{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
        "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
        "- activation functions 중 relu사용시 함수 직접 정의\n",
        "- lr, optimizer 등 바꿔보기\n",
        "- hidden layer/neuron 수를 바꾸기\n",
        "- 전처리도 추가\n",
        "- 모든 시도를 올려주세요!\n",
        "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
      ],
      "metadata": {
        "id": "sgAYo4nrw2F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "fX437IL6qbI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.datasets import load_breast_cancer, load_digits\n",
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
        "- 1) load_digits() <br>\n",
        "- 2) load_wine()"
      ],
      "metadata": {
        "id": "oxkFzBDNmWNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 종류 : load_digits\n",
        "data = load_digits()"
      ],
      "metadata": {
        "id": "FywYbfsKtjcR"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = data.data\n",
        "output = data.target"
      ],
      "metadata": {
        "id": "C2P0hqZ9yBGm"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "SggpQfSPt85C"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
        "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문 "
      ],
      "metadata": {
        "id": "bLMzf-2ntYeX"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "#input 30개 (속성이 30개)\n",
        "#y의 class는 2개 (양성과 음성)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEdiTZkrVqS",
        "outputId": "a72bbc16-a1ae-437f-e4d5-d8af57e3058b"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.,  0.,  0., 16., 12.,  1.,  0.,  0.,  0.,  0.,  6., 16., 14.,  7.,\n",
            "         0.,  0.,  0.,  0., 14., 15.,  1., 11.,  0.,  0.,  0.,  0., 16., 15.,\n",
            "         0., 14.,  1.,  0.,  0.,  1., 16., 10.,  0., 14.,  2.,  0.,  0.,  0.,\n",
            "        15., 13.,  3., 15.,  3.,  0.,  0.,  0.,  9., 16., 16., 15.,  0.,  0.,\n",
            "         0.,  0.,  0., 13., 16.,  8.,  0.,  0.], device='cuda:0')\n",
            "tensor(0, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
        "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
        "- len : observation 수를 정의하는 함수\n",
        "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
      ],
      "metadata": {
        "id": "combmxzmYFyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "y38TlgXoqV5Z"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "x8VHwnuFqino"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnmyUW2EDTK-",
        "outputId": "6dd92ab5-e93b-43da-bbf4-5c451c237d20"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1257, 64]), torch.Size([1257]))"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YBAubKdNZ3W",
        "outputId": "32de32f1-f283-4c52-91a5-d27466e8cd01"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까? \n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(64, 398, bias=True), \n",
        "          nn.ReLU(),\n",
        "          nn.Linear(398,15, bias=True),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(15,1, bias=True), \n",
        "          nn.Sigmoid()\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "C6V7a4tyq6Jc"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "07uV8RY7Yr_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(64,500, bias=True), # input_layer = 30, hidden_layer1 = 398 \n",
        "          nn.ReLU(),\n",
        "        nn.BatchNorm1d(500)\n",
        "    )\n",
        "  # activation function 이용 \n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함 \n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨 \n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨 \n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(500,200, bias=True), # hidden_layer1 = 398, hidden_layer2 = 200\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(200,100, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(100, 1, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "a0zLstbMqxEZ"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "kqcqqkECrSGK"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDUBFg6rUpw",
        "outputId": "b50fa987-2ecb-445c-980a-20ff9a9e7e69"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-240-6196a47a4462>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(layer.weight)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=500, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=200, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=1, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZt5CetrYFb",
        "outputId": "5e3ce97a-5663-45f3-e438-2caddfc91373"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=500, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=200, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=1, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "loss_fn = nn.BCELoss().to(device)\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.001)\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.5)\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "AYFp-eTErh7b"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train.reshape(-1, 1).float())\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90QxHvlIrjS7",
        "outputId": "c639ba1b-e658-4f6e-f944-f9a844dd34c1"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -4.4427313804626465\n",
            "10 -358.83306884765625\n",
            "20 -359.2659606933594\n",
            "30 -359.26800537109375\n",
            "40 -359.2680358886719\n",
            "50 -359.2680969238281\n",
            "60 -359.2680969238281\n",
            "70 -359.2680969238281\n",
            "80 -359.2680969238281\n",
            "90 -359.2680969238281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "81ASYrW7roFM",
        "outputId": "1d0ebbb7-4e01-456d-9987-235a97560f03"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = y_pred > 0.5\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "h4kJzpLErqhZ"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'sigmoid 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIKhs3Nr6Ay",
        "outputId": "fb2d0049-e4f7-4725-dcc9-efcae9e866dc"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [1.]\n",
            "sigmoid 한 후의 output은 [ True]\n",
            "accuracy는 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 2 : CNN 맛보기>"
      ],
      "metadata": {
        "id": "3RzRM7xThZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "56xqgtLxhZw6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "TzkF2bFNhcQ2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(320 ,10) ### : 알맞는 input은?\n",
        "\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = F.relu(self.mp(self.conv1(x)))\n",
        "    x = F.relu(self.mp(self.conv2(x)))\n",
        "    x = x.view(in_size, -1)\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "tLCSvgganBrH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
      ],
      "metadata": {
        "id": "lkYZ4pUdnUHc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "IzUrEM3EnXJb"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval() #model.eval() 의 기능은?\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "EFi0gYJGn2aa"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "zSvSZb_Bn4Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4958b7a-8838-4a96-99d5-6c4f48e7267d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-ee5561d9b7d4>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302719\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.302747\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.292720\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.270516\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.290206\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.262971\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.274517\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.261502\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.237900\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.204141\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.178818\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.119471\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.073574\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.993844\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.815343\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.630279\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.315123\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.185404\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.110514\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.786784\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.704326\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.625782\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.563336\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.547459\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.423890\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.486206\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.452767\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.540610\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.684886\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.564853\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.545726\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.413258\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.589814\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.287512\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.357985\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.486393\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.262099\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.426640\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.342710\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.323144\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.272937\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.207536\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.239039\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.386230\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.377146\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.451373\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.413724\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.305345\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.275938\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.513115\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.221119\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.252177\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.238612\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.254758\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.411770\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.360929\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.294221\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.258794\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.275162\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.217706\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.210607\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.231115\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.326388\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.180835\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.197708\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.159998\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.223567\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.353888\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.264040\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.297018\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.405641\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.342801\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.425250\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.186366\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.194151\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.486659\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.336930\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.203457\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.155001\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.165083\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.168266\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.217175\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.174087\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.198039\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.218000\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.297505\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.187833\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.353869\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.141905\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.165473\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.186605\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.105844\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.287990\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.204716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-f52337105c2a>:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1932, Accuracy: 9411/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.322999\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.394440\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.199066\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.295090\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.247329\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.152468\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.105232\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.102310\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.225932\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.164818\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.250849\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.115732\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.135465\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.204854\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.341570\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.118312\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.160189\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.075964\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.150013\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.266192\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.196396\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.058321\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.238127\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.143575\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.111137\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.204287\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.142910\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.128364\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.228694\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.091635\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.314663\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.090189\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.152022\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.115612\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.157903\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.174919\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.230255\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.249439\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.178748\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.142920\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.311248\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.290914\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.355056\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.096133\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.243122\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.236540\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.160471\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.207394\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.125068\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.249049\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.163700\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.310613\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.160835\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.106451\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.164813\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.144446\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.248760\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.105037\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.130771\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.231858\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.146058\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.109472\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.184930\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.058136\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.274708\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.365873\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.169408\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.245575\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.085581\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.219854\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.066929\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.146801\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.080657\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.055220\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.493674\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.161241\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.248792\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.161541\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.211650\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.047059\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.078148\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.088177\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.086031\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.146274\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.086552\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.177505\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.083344\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.116681\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.253224\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.081140\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.075628\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.037966\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.066285\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.277076\n",
            "\n",
            "Test set: Average loss: 0.1235, Accuracy: 9637/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.089830\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.082425\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.145499\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.096687\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.012580\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.125328\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.082923\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.185143\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.105422\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.178543\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.103479\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.076081\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.081441\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.192814\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.268476\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.202277\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.110127\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.143694\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.124832\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.070420\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.048285\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.126540\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.132813\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.076554\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.083135\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.194534\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.123827\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.109198\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.138165\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.092708\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.108393\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.089907\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.147722\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.073380\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.051600\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.275105\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.121279\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.156575\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.297796\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.131585\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.218149\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.176055\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.099552\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.113327\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.100772\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.077727\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.134951\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.287508\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.192277\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.116775\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.148906\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.086848\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.051321\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.095399\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.222021\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.113755\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.147662\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.084626\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.135353\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.160134\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.182454\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.317030\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.224408\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.092028\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.099010\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.089587\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.049505\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.135733\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.036856\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.059602\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.147386\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.203873\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.076394\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.018037\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.053825\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.041907\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.135278\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.093125\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.068026\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.149884\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.175729\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.122794\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.034255\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.181349\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.061718\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.403064\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.062905\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.068198\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.044201\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.042576\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.070252\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.050898\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.025494\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.188618\n",
            "\n",
            "Test set: Average loss: 0.0917, Accuracy: 9739/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.046678\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.106956\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.111321\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.123092\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.125898\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.451598\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.168915\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.137747\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.060569\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.159260\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.081708\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.142514\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.077450\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.220639\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.273252\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.059697\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.130883\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.105410\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.119152\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.111487\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.109858\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.075807\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.162291\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.032512\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.085657\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.014279\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.225559\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.081061\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.136340\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.242094\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.198314\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.036821\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.052684\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.075245\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.027065\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.119747\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.127461\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.111764\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.068181\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.071157\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.052851\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.089937\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.108268\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.248110\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.041259\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.035929\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.092573\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.028886\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.129131\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.033302\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.162675\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.148137\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.133678\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.083522\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.102897\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.153872\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.024091\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.044004\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.160660\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.112018\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.064300\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.170823\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.101088\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.128614\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.114741\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.037095\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.047989\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.054676\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.053308\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.034501\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.025114\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.058083\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.067610\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.147962\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.101505\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.079627\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.094910\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.132612\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.125925\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.145407\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.029980\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.085758\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.088878\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.113038\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.095105\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.053977\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.109600\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.012361\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.118164\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.157478\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.031452\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.232814\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.076062\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.121975\n",
            "\n",
            "Test set: Average loss: 0.0862, Accuracy: 9718/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.090555\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.123809\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.109173\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.067815\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.091243\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.044396\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.096981\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.059935\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.065642\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.052424\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.091413\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.076625\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.060713\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.123375\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.150665\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.059378\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.192717\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.038088\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.031045\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.077870\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.101438\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.156122\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.107993\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.158481\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.021938\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.031058\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.032841\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.045112\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.127479\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.107716\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.091656\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.026854\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.040989\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.104157\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.088670\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.065357\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.081929\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.078069\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.048819\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.112965\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.034975\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.146485\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.080216\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.128077\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.099586\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.102757\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.019230\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.049921\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.191897\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.059535\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.036010\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.111735\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.136850\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.060115\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.036524\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.181432\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.047299\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.271473\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.102558\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.089101\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.079184\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.072097\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.074899\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.023768\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.057016\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.039120\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.067155\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.032046\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.312331\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.231901\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.035198\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.045965\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.059633\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.107961\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.041064\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.026863\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.016076\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.029650\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.076835\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.018901\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.083566\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.022908\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.147452\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.113374\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.006508\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.022751\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.099712\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.083031\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.072192\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.116160\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.198186\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.066376\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.055439\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.070837\n",
            "\n",
            "Test set: Average loss: 0.0699, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.123788\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.095224\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.053523\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.047698\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.178374\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.059659\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.044914\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.171086\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.053324\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.097023\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.043670\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.106581\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.046143\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.130216\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.078349\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.082782\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.008851\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.016159\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.051124\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.111274\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.071758\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.012232\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.112990\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.078860\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.044667\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.070711\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.109764\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.064280\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.040168\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.134086\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.024951\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.043028\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.290449\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.093321\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.064078\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.170866\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.137819\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.122565\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.032709\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.009434\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.046421\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.057656\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.084613\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.045633\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.020270\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.051760\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.074255\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.111905\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.009004\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.102928\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.198011\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.169406\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.067317\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.051540\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.026813\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.055036\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.025259\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.030828\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.072632\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.061449\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.028694\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.109534\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.087541\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.181942\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.031578\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.097756\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.144006\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.426650\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.036752\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.003451\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.052841\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.030318\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.037176\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.051432\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.039838\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.059570\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.024182\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.250859\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.104462\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.019661\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.055410\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.121314\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.101420\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.165612\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.072863\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.018693\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.077976\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.120048\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.020570\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.044770\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.020634\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.121680\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.012160\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.108196\n",
            "\n",
            "Test set: Average loss: 0.0672, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.024505\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.041293\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.027083\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.129692\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.041422\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.010010\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.039891\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.088985\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.041498\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.048697\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.112018\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.020734\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.060543\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.019829\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.140815\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.010066\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.090792\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.255489\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.101759\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.017492\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.029149\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.016611\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.025052\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.029417\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.081313\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.067513\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.077229\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.035704\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.030378\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.032748\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.037807\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.045126\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.073924\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.025259\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.043072\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.089175\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.073936\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.014207\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.256918\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.085572\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.042551\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.271911\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.016412\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.079899\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.087938\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.113225\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.046060\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.098128\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.053846\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.080676\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.050558\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.039140\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.070726\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.070981\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.020092\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.108593\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.044664\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.089642\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.019477\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.107434\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.330970\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.036615\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.044222\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.021740\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.051043\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.014106\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.073816\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.180978\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.110060\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.052844\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.061617\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.208221\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.026306\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.142363\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.037974\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.072250\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.019405\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.025401\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.020175\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.041593\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.118686\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.064612\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.086983\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.104226\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.175199\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.013499\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.092387\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.068791\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.095381\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.074603\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.255405\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.068882\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.078905\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.037228\n",
            "\n",
            "Test set: Average loss: 0.0598, Accuracy: 9827/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.039045\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.032998\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.025770\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.040195\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.083859\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.104119\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.047769\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.047580\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.027553\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.078531\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.077418\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.053012\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.058018\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.070116\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.085989\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.041511\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.079631\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.045381\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.027321\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.035017\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.042606\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.032441\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.106806\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.065292\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.102153\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.087645\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.086499\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.063017\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.014842\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.043913\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.011733\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.044411\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.048042\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.107358\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.050269\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.133443\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.069890\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.079168\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.087460\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.140536\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.014997\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.087739\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.075286\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.125689\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.139037\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.053811\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.056582\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.079482\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.035887\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.049521\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.112397\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.094321\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.140096\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.009011\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.037800\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.063260\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.091377\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.011817\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.029309\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.196654\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.058288\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.031852\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.074638\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.039289\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.053668\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.008793\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.018686\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.006684\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.088127\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.033626\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.042147\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.041438\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.011036\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.019656\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.088217\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.064498\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.030956\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.034332\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.040954\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.046312\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.069124\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.014067\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.072298\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.013488\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.102925\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.093678\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.055573\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.051233\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.036549\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.006327\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.055675\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.204246\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.021757\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.121043\n",
            "\n",
            "Test set: Average loss: 0.0562, Accuracy: 9826/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.008089\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.019005\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.019828\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.067725\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.008952\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.034418\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.019254\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.064144\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.056058\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.039904\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.095672\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.004359\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.026887\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.012533\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.087475\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.031973\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.073059\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.100935\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.067364\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.012037\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.036680\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.093833\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.098604\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.044476\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.071585\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.165836\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.107606\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.013468\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.043277\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.060511\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.043060\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.060597\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.068535\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.074739\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.023644\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.096306\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.087991\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.075807\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.041866\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.036132\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.033423\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.027430\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.038362\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.053436\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.032187\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.083205\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.011739\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.111178\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.015239\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.049975\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.112259\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.084705\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.028448\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.070916\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.100738\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.022116\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.170284\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.026292\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.145278\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.148013\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.015758\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.005252\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.031797\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.033894\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.022237\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.310218\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.095858\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.051939\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.076482\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.069266\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.038715\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.068194\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.117117\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.019469\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.007171\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.021260\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.037933\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.090328\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.036547\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.031227\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.132858\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.020728\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.021071\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.084497\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.015384\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.056961\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.093213\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.025788\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.037645\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.070487\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.027197\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.014238\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.059801\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.016087\n",
            "\n",
            "Test set: Average loss: 0.0505, Accuracy: 9835/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jenu9r93JIrx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}