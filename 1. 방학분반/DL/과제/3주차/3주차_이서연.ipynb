{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgAYo4nrw2F4"
   },
   "source": [
    "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
    "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
    "- activation functions 중 relu사용시 함수 직접 정의\n",
    "- lr, optimizer 등 바꿔보기\n",
    "- hidden layer/neuron 수를 바꾸기\n",
    "- 전처리도 추가\n",
    "- 모든 시도를 올려주세요!\n",
    "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fX437IL6qbI-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxkFzBDNmWNk"
   },
   "source": [
    "# 1. Adam + ReLU, softmax 이용 : accuracy 가장 높음\n",
    "SGD보다 Adam이 성능 좋다는 사실 확인, lr=0.001 보다 lr=0.01이 더 높음\n",
    "\n",
    "mlp 연산이므로 hidden layer의 활성 함수를 ReLU 계열로 선택.\n",
    "기존에 주어진 softmax 그대로 사용\n",
    "\n",
    "아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
    "\n",
    "- 1) load_digits() <br>\n",
    "- 2) load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FywYbfsKtjcR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 종류 : \n",
    "cancer = load_breast_cancer()\n",
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C2P0hqZ9yBGm"
   },
   "outputs": [],
   "source": [
    "X =cancer.data\n",
    "Y =cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SggpQfSPt85C"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "  torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bLMzf-2ntYeX"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42, stratify= cancer.target, shuffle = True)\n",
    "\n",
    "x_train = torch.FloatTensor(x_train).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
    "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umEdiTZkrVqS",
    "outputId": "335674e0-2dd8-449e-b295-90b42c49c21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1620e+01, 1.8180e+01, 7.6380e+01, 4.0880e+02, 1.1750e-01, 1.4830e-01,\n",
      "        1.0200e-01, 5.5640e-02, 1.9570e-01, 7.2550e-02, 4.1010e-01, 1.7400e+00,\n",
      "        3.0270e+00, 2.7850e+01, 1.4590e-02, 3.2060e-02, 4.9610e-02, 1.8410e-02,\n",
      "        1.8070e-02, 5.2170e-03, 1.3360e+01, 2.5400e+01, 8.8140e+01, 5.2810e+02,\n",
      "        1.7800e-01, 2.8780e-01, 3.1860e-01, 1.4160e-01, 2.6600e-01, 9.2700e-02])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "#input 30개 (attributes 30개)\n",
    "#y의 class는 2개 (target)\n",
    "#dataset 정보 https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "combmxzmYFyn"
   },
   "source": [
    "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
    "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
    "- len : observation 수를 정의하는 함수\n",
    "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "y38TlgXoqV5Z"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.x_data = x_train\n",
    "    self.y_data = [[y] for y in y_train]\n",
    "#  데이터셋의 전처리를 해주는 부분\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
    "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
    "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x8VHwnuFqino"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "C6V7a4tyq6Jc"
   },
   "outputs": [],
   "source": [
    "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까? \n",
    "# hidden layer/neuron 수를 바꾸기 : 3개로 결정\n",
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(30,64, bias=True), \n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64,32, bias=True),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32,5, bias=True), \n",
    "          nn.Softmax()\n",
    "          ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07uV8RY7Yr_5"
   },
   "source": [
    "class로 구현 가능\n",
    "- init : 초기 생성 함수\n",
    "- foward : 순전파(입력값 => 예측값 의 과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "a0zLstbMqxEZ"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    " def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "          nn.Linear(30,64, bias=True), # input_layer = 30, hidden_layer1 = 64\n",
    "          nn.ReLU(),\n",
    "        nn.BatchNorm1d(64)\n",
    "    )\n",
    "\n",
    "  #layer 종류에 따른 활성함수의 추천 https://sanghyu.tistory.com/182 참조\n",
    "  # hidden layer-mlp에 적합한 ReLU, output layer binary classification에 적합한 sigmoid 사용을 결정함.\n",
    "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
    "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨 \n",
    "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨 \n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "          nn.Linear(64,32, bias=True), \n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.layer3 = nn.Sequential(\n",
    "          nn.Linear(32,10, bias=True), \n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.layer4 = nn.Sequential(\n",
    "        nn.Linear(10, 5, bias=True),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    " def forward(self,x):\n",
    "    output = self.layer1(x)\n",
    "    output = self.layer2(output)\n",
    "    output = self.layer3(output)\n",
    "    output = self.layer4(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kqcqqkECrSGK"
   },
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(layer.weight)\n",
    "        layer.bias.data.fill_(0.01)\n",
    "\n",
    "        #xavier사용\n",
    "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMDUBFg6rUpw",
    "outputId": "d77885a4-8bfa-4af9-bfaa-36c76dba40ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\candy\\AppData\\Local\\Temp/ipykernel_17276/2907414226.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(layer.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwZt5CetrYFb",
    "outputId": "fe01a0ba-90b9-4120-8f48-2f6710ed17c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AYFp-eTErh7b"
   },
   "outputs": [],
   "source": [
    "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 여러가지 optimizer 시도해보기\n",
    "# lr 바꿔보기\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
    "\n",
    "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# sgd 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90QxHvlIrjS7",
    "outputId": "9325edf8-d0c9-4dd9-c674-eae81a6405c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\candy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.654247760772705\n",
      "10 1.0174452066421509\n",
      "20 0.9702170491218567\n",
      "30 0.9518678188323975\n",
      "40 0.9437988996505737\n",
      "50 0.9373602867126465\n",
      "60 0.9352679252624512\n",
      "70 0.9292591214179993\n",
      "80 0.9284781217575073\n",
      "90 0.9248752593994141\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(100):\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  hypothesis = model(x_train)\n",
    "\n",
    "  # 비용 함수\n",
    "  cost = loss_fn(hypothesis, y_train)\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  losses.append(cost.item())\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "81ASYrW7roFM",
    "outputId": "330a40ca-8185-4c2d-d248-257a5c44b16f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4kJzpLErqhZ",
    "outputId": "9b336fde-36f4-470d-9a2f-dc21055e0353"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  model = model.to('cpu')\n",
    "  y_pred = model(x_test)\n",
    "  y_pred = y_pred.detach().numpy()\n",
    "  predicted = np.argmax(y_pred, axis =1)\n",
    "  accuracy = (accuracy_score(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyIKhs3Nr6Ay",
    "outputId": "6d73d644-c68b-4a84-defc-6f57192ef91a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model의 output은 :  [1.0000000e+00 1.3447704e-10 1.9076400e-14 3.1522357e-10 2.4614328e-11]\n",
      "argmax를 한 후의 output은 0\n",
      "accuracy는 0.9532163742690059\n"
     ]
    }
   ],
   "source": [
    "print(f'model의 output은 :  {y_pred[0]}')\n",
    "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
    "print(f'accuracy는 {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxkFzBDNmWNk"
   },
   "source": [
    "# 2. SGD+ ReLU, sigmoid이용 : accuracy 0.9298245614035088\n",
    "\n",
    "binary classification(양성, 음성) 이므로 sigmoid 사용함.\n",
    "BCELoss 를 사용하고자 하였으나 실패함\n",
    "아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
    "\n",
    "- 1) load_digits() <br>\n",
    "- 2) load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fX437IL6qbI-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "C2P0hqZ9yBGm"
   },
   "outputs": [],
   "source": [
    "X =cancer.data\n",
    "Y =cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SggpQfSPt85C"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == \"cuda\":\n",
    "  torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bLMzf-2ntYeX"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42, stratify= cancer.target, shuffle = True)\n",
    "\n",
    "x_train = torch.FloatTensor(x_train).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
    "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umEdiTZkrVqS",
    "outputId": "335674e0-2dd8-449e-b295-90b42c49c21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1620e+01, 1.8180e+01, 7.6380e+01, 4.0880e+02, 1.1750e-01, 1.4830e-01,\n",
      "        1.0200e-01, 5.5640e-02, 1.9570e-01, 7.2550e-02, 4.1010e-01, 1.7400e+00,\n",
      "        3.0270e+00, 2.7850e+01, 1.4590e-02, 3.2060e-02, 4.9610e-02, 1.8410e-02,\n",
      "        1.8070e-02, 5.2170e-03, 1.3360e+01, 2.5400e+01, 8.8140e+01, 5.2810e+02,\n",
      "        1.7800e-01, 2.8780e-01, 3.1860e-01, 1.4160e-01, 2.6600e-01, 9.2700e-02])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "#input 30개 (attributes 30개)\n",
    "#y의 class는 2개 (target)\n",
    "#dataset 정보 https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "y38TlgXoqV5Z"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.x_data = x_train\n",
    "    self.y_data = [[y] for y in y_train]\n",
    "#  데이터셋의 전처리를 해주는 부분\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
    "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
    "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "x8VHwnuFqino"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "C6V7a4tyq6Jc"
   },
   "outputs": [],
   "source": [
    "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까? \n",
    "# hidden layer/neuron 수를 바꾸기 : 2개로 결정\n",
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(30,64, bias=True), \n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64,32, bias=True),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32,5, bias=True), \n",
    "          nn.Sigmoid()\n",
    "          ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "a0zLstbMqxEZ"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    " def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "          nn.Linear(30,64, bias=True), # input_layer = 30, hidden_layer1 = 398 \n",
    "          nn.ReLU(),\n",
    "        nn.BatchNorm1d(64)\n",
    "    )\n",
    "\n",
    "  #layer 종류에 따른 활성함수의 추천 https://sanghyu.tistory.com/182 참조\n",
    "  # hidden layer-mlp에 적합한 ReLU, output layer binary classification에 적합한 sigmoid 사용을 결정함.\n",
    "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
    "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨 \n",
    "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨 \n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "          nn.Linear(64,32, bias=True), \n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.layer3 = nn.Sequential(\n",
    "          nn.Linear(32,16, bias=True), \n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.layer4 = nn.Sequential(\n",
    "        nn.Linear(16, 5, bias=True), \n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    " def forward(self,x):\n",
    "    output = self.layer1(x)\n",
    "    output = self.layer2(output)\n",
    "    output = self.layer3(output)\n",
    "    output = self.layer4(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kqcqqkECrSGK"
   },
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(layer.weight)\n",
    "        layer.bias.data.fill_(0.01)\n",
    "\n",
    "        #xavier사용\n",
    "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMDUBFg6rUpw",
    "outputId": "d77885a4-8bfa-4af9-bfaa-36c76dba40ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\candy\\AppData\\Local\\Temp/ipykernel_17276/2907414226.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(layer.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=5, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwZt5CetrYFb",
    "outputId": "fe01a0ba-90b9-4120-8f48-2f6710ed17c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=5, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "AYFp-eTErh7b"
   },
   "outputs": [],
   "source": [
    "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 여러가지 optimizer 시도해보기 :SGD보다 Adam이 성능 좋음\n",
    "# lr 바꿔보기\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# sgd 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90QxHvlIrjS7",
    "outputId": "9325edf8-d0c9-4dd9-c674-eae81a6405c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4003815650939941\n",
      "10 1.3386380672454834\n",
      "20 1.2407230138778687\n",
      "30 1.1606491804122925\n",
      "40 1.0913678407669067\n",
      "50 1.0455197095870972\n",
      "60 1.0206027030944824\n",
      "70 1.005723476409912\n",
      "80 0.9953458309173584\n",
      "90 0.9878197312355042\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(100):\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  hypothesis = model(x_train)\n",
    "\n",
    "  # 비용 함수\n",
    "  cost = loss_fn(hypothesis, y_train)\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  losses.append(cost.item())\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "81ASYrW7roFM",
    "outputId": "330a40ca-8185-4c2d-d248-257a5c44b16f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4kJzpLErqhZ",
    "outputId": "9b336fde-36f4-470d-9a2f-dc21055e0353"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  model = model.to('cpu')\n",
    "  y_pred = model(x_test)\n",
    "  y_pred = y_pred.detach().numpy()\n",
    "  predicted = np.argmax(y_pred, axis =1)\n",
    "  accuracy = (accuracy_score(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyIKhs3Nr6Ay",
    "outputId": "6d73d644-c68b-4a84-defc-6f57192ef91a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model의 output은 :  [0.12011799 0.9910533  0.02357045 0.05024213 0.04877355]\n",
      "argmax를 한 후의 output은 1\n",
      "accuracy는 0.9181286549707602\n"
     ]
    }
   ],
   "source": [
    "print(f'model의 output은 :  {y_pred[0]}')\n",
    "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
    "print(f'accuracy는 {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RzRM7xThZV_"
   },
   "source": [
    "# < 3주차 과제 2 : CNN 맛보기>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "56xqgtLxhZw6"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "TzkF2bFNhcQ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ed3fb7c174467893a3728ba4271f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c560203f90064a1a917d70dbf894d190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90ffe58db4e446f98106ab58f0b4ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a05720a06d42478fe28acdc912d5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "tLCSvgganBrH"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "    self.mp = nn.MaxPool2d(2)\n",
    "    self.fc = nn.Linear(320 , 10) ### : 알맞는 input은?\n",
    "\n",
    "  def forward(self, x):\n",
    "    in_size = x.size(0)\n",
    "    x = F.relu(self.mp(self.conv1(x)))\n",
    "    x = F.relu(self.mp(self.conv2(x)))\n",
    "    x = x.view(in_size, -1)\n",
    "    x = self.fc(x)\n",
    "    return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "lkYZ4pUdnUHc"
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "IzUrEM3EnXJb"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 10 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "EFi0gYJGn2aa"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval() #model.eval() 의 기능은?\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인!\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "zSvSZb_Bn4Nx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\candy\\AppData\\Local\\Temp/ipykernel_25056/1801410527.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311863\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.296999\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.280773\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.275717\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.260064\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.211017\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.204179\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.118759\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.041867\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.911451\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.793231\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.463733\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.187032\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.905348\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.933150\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.763955\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.736663\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.516974\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.633448\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.596964\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.601371\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.499465\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.434331\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.437664\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.406202\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.648213\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.437953\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.330471\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.383817\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.445322\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.471100\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.451223\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.326862\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.565487\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.328899\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.242902\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.341633\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.572904\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.367024\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.228175\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.601255\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.287931\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.250875\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.222324\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.366800\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.439751\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.277388\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.199058\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.686604\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.476044\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.380887\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.354916\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.280281\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.297414\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.224224\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.258536\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.367682\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.372861\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.275411\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.421466\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.327552\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.164296\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.212082\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.262597\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.250121\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.167491\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.274596\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.277950\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.092460\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.261941\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.283881\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.169104\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.197366\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.288721\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.198466\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.127874\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.176994\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.228066\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.074036\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.281243\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.198989\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.315930\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.166928\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.177372\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.165011\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.165410\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.174310\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.096740\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.111670\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.188978\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.165892\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.170734\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.496861\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.266175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\candy\\AppData\\Local\\Temp/ipykernel_25056/4274826149.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "C:\\Users\\candy\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1881, Accuracy: 9447/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.143864\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.314665\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.099848\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.299140\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.117232\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.209631\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.345967\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.092245\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.174963\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.200343\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.161200\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.159713\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.299889\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.075099\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.275512\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.251022\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.042217\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.190252\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.139700\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.171112\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.131909\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.159584\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.108859\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.400457\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.184770\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.161922\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.091489\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.240128\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.178339\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.149332\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.264876\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.079845\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.312442\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.276836\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.145279\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.106747\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.293433\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.333470\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.099354\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.181743\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.074936\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.191261\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.223686\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.119803\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.144293\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.192283\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.109478\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.090393\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.275250\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.153237\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.064343\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.289479\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.282620\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.181320\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.205448\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.316283\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.095243\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.146893\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.061863\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.121230\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.262613\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.138447\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.125193\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.143397\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.101749\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.197811\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.195945\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.036425\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.128508\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.199349\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.124966\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.072655\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.119901\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.203288\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.190238\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.135236\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.132988\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.103429\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.102233\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.103302\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.104420\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.117716\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.041796\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.124702\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.143245\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.086453\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.155562\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.220222\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.082094\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.305845\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.183165\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.110598\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.134157\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.067362\n",
      "\n",
      "Test set: Average loss: 0.1087, Accuracy: 9685/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.096337\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.073632\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.098518\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.046648\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.178879\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.134345\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.123522\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.097384\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.203660\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.126587\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.094821\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.237023\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.204060\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.148691\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.130729\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.070327\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.121703\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.194624\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.193936\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.132040\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.147538\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.051082\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.201838\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.034940\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.101626\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.138659\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.056785\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.058530\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.162763\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.056282\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.137669\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.222857\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.028537\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.215680\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.110593\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.086606\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.042341\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.198415\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.160020\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.099719\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.101816\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.210424\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.021705\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.062139\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.094472\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.084038\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.052231\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.114275\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.079263\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.091509\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.089737\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.038129\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.196199\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.014943\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.130728\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.063174\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.173161\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.127961\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.134808\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.249665\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.101524\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.056364\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.335988\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.092956\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.159705\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.022352\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.076742\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.055900\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.057955\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.105140\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.098771\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.129842\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.081754\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.219067\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.034841\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.108984\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.100494\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.082446\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.113353\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.143203\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.041556\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.056479\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.131084\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.159407\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.246194\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.051030\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.038719\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.089411\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.022720\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.051559\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.047570\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.190986\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.050582\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.250052\n",
      "\n",
      "Test set: Average loss: 0.0867, Accuracy: 9754/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.109184\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.124782\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.242969\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.224257\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.108397\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.036205\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.108555\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.058465\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.092675\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.140123\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.061686\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.175947\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.057747\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.058744\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.099601\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.110354\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.050389\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.074952\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.061304\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.236452\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.102607\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.018674\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.135317\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.046470\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.186178\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.045730\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.095294\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.144007\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.114338\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.028618\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.127934\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.051918\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.047186\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.078573\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.170568\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.087158\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.111044\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.059811\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.174120\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.102805\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.128617\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.049028\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.033117\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.042450\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.091230\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.104402\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.114401\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.062867\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.032626\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.107551\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.113879\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.061618\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.082062\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.030517\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.048539\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.087705\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.063741\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.147055\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.059573\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.088821\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.157442\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.098508\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.064565\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.157156\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.100959\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.097699\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.163185\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.043912\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.059521\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.058878\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.133033\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.024904\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.196898\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.057100\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.100967\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.072800\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.016509\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.101488\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.032529\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.043283\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.030022\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.083032\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.052198\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.130468\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.262748\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.153724\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.049568\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.102668\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.072498\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.077644\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.090379\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.035293\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.296293\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.085144\n",
      "\n",
      "Test set: Average loss: 0.0791, Accuracy: 9772/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.077504\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.071653\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.150711\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.097696\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.054478\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.104098\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.035294\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.111075\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.117684\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.119991\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.191685\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.085200\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.030200\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.124203\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.053179\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.079945\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.192364\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.047636\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.035573\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.053635\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.156150\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.109008\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.060419\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.050504\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.051763\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.056762\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.075293\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.148909\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.034384\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.037151\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.032544\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.029186\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.250604\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.024820\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.067912\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.053814\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.062071\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.047414\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.039269\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.123440\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.093314\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.021763\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.065938\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.088692\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.118226\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.099999\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.024494\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.038267\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.121240\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.055630\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.084648\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.097488\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.306526\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.055200\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.170001\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.066833\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.080562\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.113622\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.186462\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.037961\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.132066\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.071198\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.091461\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.122157\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.129959\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.298868\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.117216\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.065782\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.007091\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.027198\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.052790\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.069851\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.049479\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.075345\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.044889\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.093439\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.101162\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.032720\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.128041\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.316179\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.052276\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.294354\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.087533\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.029782\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.111855\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.057227\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.185770\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.047602\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.028844\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.246883\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.086976\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.027751\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.152694\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.037911\n",
      "\n",
      "Test set: Average loss: 0.0720, Accuracy: 9786/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.047453\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.014369\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.061387\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.134937\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.062617\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.135659\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.110482\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.050936\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.059662\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.021079\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.152314\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.034329\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.088345\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.103138\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.027337\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.096363\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.187811\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.036518\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.028428\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.097841\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.086032\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.012054\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.063175\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.077056\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.068319\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.039049\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.034122\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.013506\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.192769\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.042452\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.037074\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.129539\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.060907\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.051186\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.047097\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.037889\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.018050\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.121424\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.047266\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.188611\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.016924\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.065552\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.073662\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.084716\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.088312\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.030294\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.051924\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.181283\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.121088\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.161631\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.026593\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.034217\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.107640\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.079373\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.131537\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.012412\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.041557\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.152267\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.173948\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.051970\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.012159\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.111583\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.024963\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.062271\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.032116\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.015626\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.023795\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.114280\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.115944\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.065535\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.025762\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.046681\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.121760\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.069193\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.103080\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.140201\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.192343\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.080192\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.013215\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.071861\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.153202\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.044678\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.012231\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.088032\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.073868\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.177678\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.045873\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.023151\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.058941\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.117785\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.050256\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.041450\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.118678\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.063913\n",
      "\n",
      "Test set: Average loss: 0.0558, Accuracy: 9833/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.033421\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.046720\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.041239\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.083772\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.029650\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.181520\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.206872\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.038602\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.012871\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.065940\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.086991\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.020538\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.051006\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.066002\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.028470\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.038136\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.067396\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.046604\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.012680\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.029582\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.108891\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.097448\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.115251\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.059099\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.033643\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.016670\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.063003\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.098422\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.038487\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.171238\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.017168\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.010988\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.103673\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.062956\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.008922\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.368356\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.023796\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.170620\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.075295\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.080115\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.154154\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.106471\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.135943\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.026062\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.025311\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.045808\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.064447\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.103605\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.161274\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.009191\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.044673\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.143313\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.081206\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.025848\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.044621\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.040831\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.053796\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.025802\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.043628\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.071749\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.059525\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.035457\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.033566\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.099824\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.125850\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.058707\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.046570\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.028498\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.075432\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.054293\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.047925\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.058118\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.025487\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.043048\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.053218\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.137950\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.093251\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.058678\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.017025\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.020475\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.044055\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.118012\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.039671\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.012597\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.014638\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.071994\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.037386\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.011844\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.036119\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.077565\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.015895\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.034724\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.087013\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.166152\n",
      "\n",
      "Test set: Average loss: 0.0566, Accuracy: 9818/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.019512\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.043910\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.057944\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.027292\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.080662\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.071054\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.106246\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.019654\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.060278\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.077474\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.016727\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.013067\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.130054\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.027991\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.041085\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.044214\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.055403\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.083838\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.008459\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.146017\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.084409\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.120119\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.033086\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.131174\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.206012\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.111390\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.025526\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.062783\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.046521\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.013152\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.207648\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.021421\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.052411\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.077381\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.227942\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.013182\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.031403\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.055879\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.017977\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.070598\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.137155\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.037514\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.042244\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.035567\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.051559\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.024672\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.134348\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.042673\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.014606\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.020097\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.086239\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.112834\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.052345\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.077812\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.013388\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.035543\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.191524\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.101067\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.078308\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.059741\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.025744\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.049243\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.053197\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.058027\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.124520\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.016245\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.026556\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.102532\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.011447\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.099200\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.083661\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.109719\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.032537\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.024200\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.010900\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.015058\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.046366\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.108776\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.021544\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.035136\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.034899\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.055699\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.054651\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.079882\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.004841\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.313243\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.111659\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.061834\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.022151\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.032332\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.024972\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.141836\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.129837\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.085374\n",
      "\n",
      "Test set: Average loss: 0.0528, Accuracy: 9833/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.015900\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.039318\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.098311\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.157081\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.071589\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.058133\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.037856\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.068836\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.097645\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.019787\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.069118\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.144297\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.081718\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.044474\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.126119\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.052091\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.091557\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.090902\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.060174\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.073948\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.043046\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.006736\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.099844\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.039480\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.040058\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.021074\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.018950\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.028353\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.037658\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.174731\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.039997\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.061274\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.118276\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.101750\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.018772\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.088642\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.012183\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.043661\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.035595\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.104660\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.018705\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.098983\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.055120\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.156712\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.008137\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.031277\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.114359\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.086995\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.040337\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.159243\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.033489\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.069804\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.034279\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.085814\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.064260\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.048539\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.105854\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.008212\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.011780\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.048474\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.009936\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.051007\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.019362\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.051692\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.038262\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.075143\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.020226\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.080335\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.053857\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.054029\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.113306\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.115588\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.030742\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.027132\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.021554\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.012188\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.064066\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.027700\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.098286\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.101343\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.086355\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.010331\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.088568\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.025855\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.108752\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.016534\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.020570\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.114435\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.082791\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.022846\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.019828\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.083436\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.002409\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.049333\n",
      "\n",
      "Test set: Average loss: 0.0575, Accuracy: 9808/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
