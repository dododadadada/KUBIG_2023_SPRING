{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 1 : MLP 마음대로 다뤄보기>\n",
        "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
        "- activation functions 중 relu사용시 함수 직접 정의\n",
        "- lr, optimizer 등 바꿔보기\n",
        "- hidden layer/neuron 수를 바꾸기\n",
        "- 전처리도 추가\n",
        "- 모든 시도를 올려주세요!\n",
        "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
      ],
      "metadata": {
        "id": "sgAYo4nrw2F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0s20GFUmYSt7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 아래 데이터셋 중 원하는 데이터셋 하나를 선택하여, 코드 과제 진행하기!\n",
        "- 1) load_digits() <br>\n",
        "- 2) load_wine()"
      ],
      "metadata": {
        "id": "oxkFzBDNmWNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 종류 :\n",
        "from sklearn.datasets import load_digits,load_wine\n",
        "data = load_digits()"
      ],
      "metadata": {
        "id": "FywYbfsKtjcR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = data.data\n",
        "output = data.target"
      ],
      "metadata": {
        "id": "C2P0hqZ9yBGm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input.shape, output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drBSgaoCT3yE",
        "outputId": "ab2a99c8-eaae-44a7-e3bf-f840ed42704c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1797, 64), (1797,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "SggpQfSPt85C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
        "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문 "
      ],
      "metadata": {
        "id": "bLMzf-2ntYeX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "#input 30개 (속성이 30개)\n",
        "#y의 class는 2개 (양성과 음성)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEdiTZkrVqS",
        "outputId": "b7c5152b-2734-4a43-9389-650e9301dd68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.,  0.,  0., 16., 12.,  1.,  0.,  0.,  0.,  0.,  6., 16., 14.,  7.,\n",
            "         0.,  0.,  0.,  0., 14., 15.,  1., 11.,  0.,  0.,  0.,  0., 16., 15.,\n",
            "         0., 14.,  1.,  0.,  0.,  1., 16., 10.,  0., 14.,  2.,  0.,  0.,  0.,\n",
            "        15., 13.,  3., 15.,  3.,  0.,  0.,  0.,  9., 16., 16., 15.,  0.,  0.,\n",
            "         0.,  0.,  0., 13., 16.,  8.,  0.,  0.], device='cuda:0')\n",
            "tensor(0, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 베이스라인 기반 시도: 0.985\n",
        "Activation Function = Sigmoid, lr = 0.01, Optimizer = Adam, layer: 64-398-40-25-10"
      ],
      "metadata": {
        "id": "Hx_dpVgLXKyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
        "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
        "- len : observation 수를 정의하는 함수\n",
        "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
      ],
      "metadata": {
        "id": "combmxzmYFyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "y38TlgXoqV5Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "x8VHwnuFqino"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까? \n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(64,398, bias=True), \n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(398,40, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(40,15, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(40,15, bias=True), \n",
        "          nn.Softmax()\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "C6V7a4tyq6Jc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "07uV8RY7Yr_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(64,398, bias=True), # input_layer = 30, hidden_layer1 = 398 \n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(398)\n",
        "    )\n",
        "  # activation function 이용 \n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함 \n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨 \n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨 \n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(398,40, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(40,15, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(15, 10, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "a0zLstbMqxEZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "kqcqqkECrSGK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to('cuda')\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDUBFg6rUpw",
        "outputId": "9fb9ad4c-7930-4311-dfa5-d949d7b3a7d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-6196a47a4462>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(layer.weight)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=398, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=398, out_features=40, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=40, out_features=15, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZt5CetrYFb",
        "outputId": "6580786e-adb7-4de2-ac2c-3c51fc77c26e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=398, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): BatchNorm1d(398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=398, out_features=40, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=40, out_features=15, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=15, out_features=10, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "AYFp-eTErh7b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90QxHvlIrjS7",
        "outputId": "3b70bdc0-daa2-43a8-9ce8-2c5c5e490a9d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.3035693168640137\n",
            "10 2.158271551132202\n",
            "20 1.9978446960449219\n",
            "30 1.8248190879821777\n",
            "40 1.6843771934509277\n",
            "50 1.5944284200668335\n",
            "60 1.5361051559448242\n",
            "70 1.5052266120910645\n",
            "80 1.4910691976547241\n",
            "90 1.4836788177490234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "81ASYrW7roFM",
        "outputId": "57fa6c22-4824-405e-cc0e-5431e9d83b20"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "h4kJzpLErqhZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIKhs3Nr6Ay",
        "outputId": "0ad9b272-aded-4426-a631-f5ed228ab8b8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [7.0717568e-03 9.6824372e-01 1.0486693e-02 1.9104435e-03 6.0859852e-04\n",
            " 2.8201600e-03 1.7693198e-03 1.7478563e-03 5.2667097e-03 7.4706411e-05]\n",
            "argmax를 한 후의 output은 1\n",
            "accuracy는 0.9851851851851852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 추가 시도(1): 0.180\n",
        "Activation Function = Sigmoid, lr = 0.4, Optimizer = Sgd, layer: 64-256-96-32-20-10, batch size = 10\n",
        "- layer 수를 과하게 추가하면 심각한 성능 저하로 이어짐.\n",
        "- Sgd Optimizer를 사용했을 시 Adam보다는 높은 학습률을 설정해야 accuracy를 높일 수 있음."
      ],
      "metadata": {
        "id": "drINQDE2XSD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "FzB71Ns9XSD-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "fxqao-kwXSD-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까? \n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(64,256, bias=True), \n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(256,96, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(96,32, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(32,20, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(32,20, bias=True), \n",
        "          nn.Softmax()\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "AL6cKkIrXSD-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(64,256, bias=True), # input_layer = 30, hidden_layer1 = 398 \n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(256)\n",
        "    )\n",
        "  # activation function 이용 \n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함 \n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨 \n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨 \n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(256,96, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(96,32, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "          nn.Linear(32,20, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer5 = nn.Sequential(\n",
        "        nn.Linear(20, 10, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "6tBTm7ArXSD_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "Y-aEhfISXSD_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to('cuda')\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b52d7ae-fb21-41e4-9a05-e6c282922378",
        "id": "bLs6qM5-XSD_"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-6196a47a4462>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(layer.weight)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=96, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=96, out_features=32, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=32, out_features=20, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer5): Sequential(\n",
              "    (0): Linear(in_features=20, out_features=10, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d660f6-1d46-4cf0-b6bd-d4f48e8b33e9",
        "id": "DaoonBAMXSD_"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=96, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=96, out_features=32, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=32, out_features=20, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer5): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=10, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.4, momentum=0.9)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "gRAUBPmoXSD_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc901d39-70d1-424b-ff83-0f9fb77a429c",
        "id": "AdPuCtb-XSD_"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3.0230565071105957\n",
            "10 2.6642613410949707\n",
            "20 2.6171157360076904\n",
            "30 2.614184856414795\n",
            "40 2.6118903160095215\n",
            "50 2.6056299209594727\n",
            "60 2.584429979324341\n",
            "70 2.5529682636260986\n",
            "80 2.5178182125091553\n",
            "90 2.487434148788452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fd0b37f0-7f22-4d95-8093-073f5e84ef83",
        "id": "brv_wodPXSEA"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "7J-NAZEzXSEA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf000e7-f87d-4bfc-cad7-60d7fb849dec",
        "id": "0UQo8k4WXSEA"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [0.02511201 0.96296185 0.91518736 0.99520844 0.02272543 0.9879325\n",
            " 0.07707392 0.15884174 0.98254955 0.9927859  0.00718358 0.00364751\n",
            " 0.00408792 0.0075676  0.00434527 0.00607891 0.00453391 0.00517145\n",
            " 0.00590696 0.00589742]\n",
            "argmax를 한 후의 output은 3\n",
            "accuracy는 0.1425925925925926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 추가 시도(2) : 0.987, Accuracy 가장 높은 시도\n",
        "Activation Function = Relu, lr = 0.006, Optimizer = Adam, layer: 64-512-48-24-10\n",
        "- layer의 개수 및 노드 수가 늘어날수록 Sigmoid와 ReLU의 성능 차이 발생하는 것으로 파악됨. (같은 조건에서 활성함수를 모두 Sigmoid로 변경하면 0.95 수준)\n",
        "- Adam Optimizer의 경우 학습률을 높이는 것이 성능에 긍정적으로 작용하지 않는 듯함."
      ],
      "metadata": {
        "id": "o5mUcGW4c_j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]\n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device)\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "EKn9xqyLc_kB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "zL2LVkwPc_kB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_layer와 hidden_layer의 노드 개수를 어떻게 바꿔줘야할까? \n",
        "# hidden layer/neuron 수를 바꾸기\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Linear(64,512, bias=True), \n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512,48, bias=True),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(48,24, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(48,24, bias=True), \n",
        "          nn.Softmax()\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "ZvHMTG_sc_kB"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(64,512, bias=True), # input_layer = 30, hidden_layer1 = 398 \n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(512)\n",
        "    )\n",
        "  # activation function 이용 \n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함 \n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨 \n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨 \n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(512,48, bias=True), # hidden_layer1 = 398, hidden_layer2 = 15\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(48,24, bias=True), # hidden_layer1 = 15, hidden_layer2 = 10\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(24, 10, bias=True), # hidden_layer3 = 10, output_layer = 5\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "xA9i4Ftyc_kB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.005)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서"
      ],
      "metadata": {
        "id": "KqRV35TYc_kB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to('cuda')\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce592208-fa54-4b88-8df5-e77edf538f10",
        "id": "UZzAIYdzc_kB"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-b0d97ce66a00>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(layer.weight)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=512, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=48, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=48, out_features=24, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=24, out_features=10, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d422d5be-89fc-4627-b84c-b94d23519037",
        "id": "jZ0Wrm1zc_kC"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=512, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=48, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=48, out_features=24, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=24, out_features=10, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# 여러가지 optimizer 시도해보기\n",
        "# lr 바꿔보기\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.006)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "FFE7fR4nc_kC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b36a60f-dfc4-46ce-cf4b-7335f460d952",
        "id": "WxQYEulHc_kC"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.302907943725586\n",
            "10 2.125934362411499\n",
            "20 1.978169560432434\n",
            "30 1.8431599140167236\n",
            "40 1.7462273836135864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 1.6809332370758057\n",
            "60 1.6213123798370361\n",
            "70 1.5613346099853516\n",
            "80 1.5179235935211182\n",
            "90 1.4952698945999146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eac88289-7acc-407d-dafd-e5e545b283a3",
        "id": "qF4NxFAPc_kC"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "id": "PD1fHg53c_kC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 :  {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2fa49d-9ea4-413c-e147-7147eec63161",
        "id": "Ubay4hVic_kC"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 :  [4.1470505e-04 9.7847956e-01 2.7619081e-03 1.7645246e-03 7.8640357e-03\n",
            " 3.4478036e-04 6.8527652e-04 3.0345605e-03 3.9473246e-03 7.0334924e-04]\n",
            "argmax를 한 후의 output은 1\n",
            "accuracy는 0.987037037037037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# < 3주차 과제 2 : CNN 맛보기>"
      ],
      "metadata": {
        "id": "3RzRM7xThZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "56xqgtLxhZw6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "TzkF2bFNhcQ2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(320 , 10) ### : 알맞는 input은? 320!\n",
        "\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = F.relu(self.mp(self.conv1(x)))\n",
        "    x = F.relu(self.mp(self.conv2(x)))\n",
        "    x = x.view(in_size, -1)\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "tLCSvgganBrH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
      ],
      "metadata": {
        "id": "lkYZ4pUdnUHc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "IzUrEM3EnXJb"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval() #model.eval() 의 기능은? \n",
        "    # 모델 평가(evaluation) 모드로 전환하는 기능. Test set을 모델이 학습하지 않고 예측만 하도록 설정함.\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data # nll_loss?? / cross entropy loss와의 관계 확인! \n",
        "        # nll_loss는 이진 분류에서 음의 로그 우도로, cross entropy loss와 비교했을 때 softmax 부분이 빠져있다고 할 수 있음.\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "EFi0gYJGn2aa"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "zSvSZb_Bn4Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64a2feb-be11-4149-cee3-76cd264295e8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-fc4d253827be>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304631\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.298917\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.289230\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.276697\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.266912\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.243018\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.242181\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.176081\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.077912\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.066079\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.868844\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.717011\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.402820\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.302919\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.992462\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.957060\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.779702\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.710733\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.690252\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.614448\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.573204\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.499547\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.302053\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.385899\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.366619\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.468093\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.461934\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.423004\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.670402\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.340571\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.542491\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.549847\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.364114\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.400824\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.386170\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.666500\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.478303\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.499113\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.384526\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.563399\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.226433\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.437106\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.428504\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.378798\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.362777\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.461191\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.186647\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.212300\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.159666\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.170925\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.456090\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.169725\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.543359\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.411025\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.317785\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.243607\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.386603\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.318230\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.348535\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.214348\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.430583\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.165709\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.130052\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.357401\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.173402\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.212776\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.317798\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.233746\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.390518\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.167540\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.108108\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.164005\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.146970\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.336977\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.288522\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.144523\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.367422\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.177435\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.235476\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.137464\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.149177\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.169728\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.095640\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.173502\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.300685\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.164897\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.166560\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.101602\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.176434\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.127018\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.369770\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.292381\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.095056\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.121342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-2e9a1742d671>:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1856, Accuracy: 9436/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.207066\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.216108\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.354286\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.114324\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.160911\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.175837\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.228003\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.205816\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.276080\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.134030\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.236736\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.330650\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.105971\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.088248\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.097707\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.127805\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.174943\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.149159\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.078565\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.177552\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.211692\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.287884\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.109244\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.168111\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.195005\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.173625\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.163272\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.210363\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.239374\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.122504\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.151550\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.108145\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.117333\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.230524\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.101674\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.074963\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.159955\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.104762\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.141908\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.174800\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.148554\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.165473\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.124091\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.274876\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.102911\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.123573\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.110348\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.185777\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.157992\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.173857\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.099076\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.276164\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.463350\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.283631\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.311060\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.228072\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.199384\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.095214\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.154733\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.399175\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.102932\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.145657\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.087657\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.124756\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.095432\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.186734\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.062999\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.135644\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.066925\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.229141\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.086446\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.092402\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.085578\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.165577\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.064156\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.266031\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.214963\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.340426\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.073673\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.063101\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.149544\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.122000\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.093256\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.099627\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.072593\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.084404\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.032025\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.105948\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.233084\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.036553\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.158959\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.233825\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.037030\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.127285\n",
            "\n",
            "Test set: Average loss: 0.1274, Accuracy: 9613/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.222838\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.103771\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.239289\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.227400\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.050369\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.109936\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.119920\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.135022\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.106364\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.081165\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.058706\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.113069\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.160928\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.118706\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.151194\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.100122\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.133939\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.040822\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.090259\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.220791\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.180910\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.254467\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.151955\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.120355\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.118650\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.078844\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.112289\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.188822\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.053912\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.073456\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.093050\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.233183\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.152584\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.060215\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.109720\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.271207\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.088808\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.036258\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.089004\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.064841\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.050071\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.180990\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.140648\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.077576\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.048879\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.216085\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.056990\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.039730\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.180548\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.098566\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.150154\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.112227\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.158385\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.160390\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.048035\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.142028\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.112969\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.103090\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.077345\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.181562\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.083282\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.251676\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.055969\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.147673\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.125322\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.042925\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.090457\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.176147\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.102021\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.060056\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.117739\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.089742\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.055845\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.182091\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.151848\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.059735\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.102439\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.055568\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.064537\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.261126\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.057159\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.059105\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.125570\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.144090\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.114296\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.117825\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.063815\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.084547\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.078352\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.064681\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.147467\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.044979\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.142873\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.183421\n",
            "\n",
            "Test set: Average loss: 0.1115, Accuracy: 9632/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.042529\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.095279\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.054293\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.102427\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.186990\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.087876\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.041291\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.040239\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.100198\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.056197\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.126444\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.013832\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.192614\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.097145\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.058377\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.155083\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.134862\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.038771\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.077165\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.096714\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.095581\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.068684\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.047495\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.076744\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.217580\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.098369\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.155313\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.106736\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.109664\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.164036\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.053766\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.232287\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.033766\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.103092\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.064145\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.089343\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.122610\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.027310\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.069935\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.138359\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.060646\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.029460\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.040961\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.088695\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.029893\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.200004\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.345988\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.085252\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.059208\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.023750\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.058938\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.082146\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.151006\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.170040\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.046835\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.082297\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.143767\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.119800\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.062455\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.123767\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.025548\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.048543\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.086421\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.042180\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.089017\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.127718\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.097767\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.087564\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.084125\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.065329\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.050219\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.107410\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.170477\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.018738\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.087210\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.178130\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.104650\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.098929\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.047937\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.048085\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.085213\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.094068\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.046240\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.031143\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.168031\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.047382\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.109617\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.047091\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.195292\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.037486\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.094869\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.042555\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.156452\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.096230\n",
            "\n",
            "Test set: Average loss: 0.0820, Accuracy: 9747/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.033606\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.073794\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.146868\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.140904\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.076590\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.049892\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.152499\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.056426\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.047841\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.031950\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.242131\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.111412\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.046002\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.128338\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.072986\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.172034\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.047511\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.137769\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.089382\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.109325\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.058439\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.035933\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.251751\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.046683\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.214134\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.086061\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.173041\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.050749\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.063628\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.082737\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.171316\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.155749\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.103869\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.063565\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.078587\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.063663\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.013062\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.060905\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.038567\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.187335\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.095137\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.047137\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.098796\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.077631\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.112860\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.155896\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.082711\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.104465\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.040048\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.067454\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.171518\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.028284\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.045962\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.026883\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.054695\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.204202\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.046504\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.159082\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.034296\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.090154\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.214604\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.048884\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.226418\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.180117\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.066583\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.091232\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.034943\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.083487\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.102312\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.201991\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.030847\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.035202\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.048035\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.063240\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.121478\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.032119\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.084743\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.157472\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.049018\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.075499\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.139050\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.087705\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.033004\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.074884\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.082783\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.046343\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.282371\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.047680\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.114299\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.027089\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.070140\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.102343\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.055871\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.043081\n",
            "\n",
            "Test set: Average loss: 0.0842, Accuracy: 9746/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.055071\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.030172\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.056476\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.013658\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.038163\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.050153\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.049289\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.257202\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.088223\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.055179\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.055407\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.057916\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.149971\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.072738\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.022418\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.098743\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.113761\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.090351\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.043522\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.013562\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.050219\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.100469\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.076145\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.037646\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.078131\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.070063\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.027855\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.222994\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.027770\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.106471\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.138750\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.037001\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.196085\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.024713\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.035231\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.090186\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.062629\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.096186\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.037013\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.049350\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.046541\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.019185\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.225871\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.090735\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.042341\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.115544\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.101283\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.074363\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.044358\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.115684\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.037381\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.055738\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.037350\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.128377\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.030241\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.045086\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.121564\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.077832\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.159105\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.046827\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.055729\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.004569\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.088395\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.123244\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.105661\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.087297\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.016990\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.178456\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.070217\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.084406\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.036528\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.051459\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.122353\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.086331\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.105712\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.009324\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.171845\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.083219\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.100346\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.015372\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.235357\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.027819\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.061379\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.048609\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.055161\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.107218\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.138180\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.336501\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.072577\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.030875\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.100916\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.137273\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.047943\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.070054\n",
            "\n",
            "Test set: Average loss: 0.0682, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.107588\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.239468\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.054552\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.147388\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.073021\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.112792\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.058763\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.024477\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.242018\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.011775\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.064274\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.141853\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.037185\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.137616\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.028565\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.016796\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.040580\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.067940\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.114703\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.165651\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.088638\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.032925\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.135222\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.052899\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.171363\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.081368\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.122222\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.078114\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.087670\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.124179\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.085767\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.059682\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.038566\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.120522\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.041774\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.062446\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.111059\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.349640\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.056374\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.052408\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.039490\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.078918\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.030205\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.149980\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.088006\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.047418\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.058998\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.052283\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.045517\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.094841\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.057212\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.105175\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.068977\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.011097\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.113349\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.018082\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.041166\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.007631\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.104954\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.113117\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.059844\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.112009\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.141990\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.043332\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.072441\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.050927\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.143713\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.154883\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.129773\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.035132\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.111893\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.038310\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.070151\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.210507\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.033286\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.220883\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.112989\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.039222\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.079485\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.043723\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.091962\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.053955\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.135125\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.046637\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.084693\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.111490\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.061613\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.152408\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.080440\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.027829\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.061543\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.073747\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.052900\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.094965\n",
            "\n",
            "Test set: Average loss: 0.0637, Accuracy: 9816/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.033123\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.087017\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.107861\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.075064\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.014681\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.018026\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.042272\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.027226\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.066149\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.058412\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.114511\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.070992\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.090926\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.119702\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.034937\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.042540\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.108102\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.036393\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.028362\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.021076\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.046079\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.091046\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.021266\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.112994\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.092553\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.058609\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.112109\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.021025\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.073408\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.024187\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.054255\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.008927\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.167696\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.097053\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.140266\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.043567\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.022247\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.076505\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.040908\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.030384\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.023536\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.046143\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.069945\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.018757\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.097236\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.012781\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.037585\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.025002\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.075771\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.018064\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.125889\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.238968\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.032748\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.160320\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.028432\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.078241\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.055579\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.049637\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.059598\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.014737\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.038383\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.162992\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.047635\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.047149\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.061360\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.030552\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.036613\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.104906\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.176603\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.175547\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.079977\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.032512\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.009543\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.008584\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.083760\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.082340\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.080236\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.175664\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.023667\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.080074\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.118703\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.062641\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.021716\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.061483\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.033289\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.099065\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.019142\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.057621\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.040696\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.049336\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.136849\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.083370\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.022772\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.204829\n",
            "\n",
            "Test set: Average loss: 0.0582, Accuracy: 9817/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.027297\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.078513\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.026689\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.132339\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.024772\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.053874\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.066996\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.030599\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.081379\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.015511\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004902\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.009615\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.049063\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.009519\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.064559\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.147481\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.035073\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.053076\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.022276\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.007810\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.009495\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.069736\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.137677\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.027232\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.009554\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.152675\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.098644\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.139242\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.028643\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.124456\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.183205\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.104364\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.035080\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.100850\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.007633\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.050016\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.035655\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.057587\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.074335\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.108825\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.103329\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.101171\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.040235\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.031175\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.047288\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.019867\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.113107\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.030652\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.070381\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.087062\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.072838\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.011226\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.133825\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.078478\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.082209\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.021082\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.074339\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.072555\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.090888\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.037530\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.073498\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.266354\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.172251\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.215680\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.046323\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.042501\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.024695\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.131707\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.134154\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.054847\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.040397\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.019309\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.100451\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.096517\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.063917\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.061631\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.049428\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.025282\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.029337\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.041487\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.038386\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.098835\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.056043\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.048653\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.160058\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.018449\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.062083\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.017414\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.007845\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.052039\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.092982\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.052073\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.037644\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.015722\n",
            "\n",
            "Test set: Average loss: 0.0546, Accuracy: 9824/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}