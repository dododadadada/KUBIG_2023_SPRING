{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nmQ5F7UAeKB_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task1\n",
        "\n",
        "빈 칸을 채워주세요!\n",
        "\n",
        "단계별 output이 github 파일에는 남아있으니 그 output과 동일한 형태인지 확인하면서 진행해주시면 됩니다~"
      ],
      "metadata": {
        "id": "Sgxd6SxmeVcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 생성할 문장 데이터\n",
        "\n",
        "sentence = (\"Brick walls are there for a reason and you must not think \"\n",
        "            \"that the brick walls aren't there to keep us out, but rather \"\n",
        "            \"in this way that the brick walls are there to show us how badly we want things.\")"
      ],
      "metadata": {
        "id": "NDvUeC8BoUb6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. 문자 집합 만들기\n",
        "world_set = list(set(sentence))\n",
        "\n",
        "## 문제(1): 각 문자에 정수 인코딩 (공백도 하나의 원소로 포함)\n",
        "vocab = {w: i for i, w in enumerate(world_set)}"
      ],
      "metadata": {
        "id": "b9lkrKyZf8ie"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_0we5Y-gYDq",
        "outputId": "3579cd40-79c3-4a10-b9d8-7b5776345fc3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'m': 0, 'i': 1, ' ': 2, 'p': 3, 't': 4, 'e': 5, 'v': 6, 'l': 7, 'c': 8, 'a': 9, 'o': 10, 'C': 11, 'k': 12, 'A': 13, 'u': 14, 'n': 15, 'G': 16, 'U': 17, 'h': 18, 'P': 19, 'r': 20, 'b': 21, 'f': 22, 's': 23, 'g': 24, ',': 25, 'T': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. 문자 집합 크기 확인\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('문자 집합 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpKupU6lgpfT",
        "outputId": "36ff547c-a688-4891-87db-e7352de2269e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 크기 : 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
        "\n",
        "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
        "sequence_length = 10  # 너무 길거나 너무 짧게 잡으면 안됩니다!\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "wFDZJHSMg9In"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. seqence 길이 단위 자르기\n",
        "\n",
        "# 데이터 구성을 위한 리스트\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "## 문제(2): 반복문 내에서의 인덱싱을 사용하여 sequence_length 값 단위로 샘플을 잘라 데이터 만들기, y_str은 x_str은 한 칸씩 쉬프트된 sequnce\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "  x_str = sentence[i:i + sequence_length]\n",
        "  y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "  print(i, x_str, \"->\", y_str)\n",
        "\n",
        "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
        "  x_data.append([vocab[c] for c in x_str])\n",
        "  y_data.append([vocab[d] for d in y_str])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbDcmJmghN7V",
        "outputId": "ffcc5157-2843-4cce-be86-71d65bf01efc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Although t -> lthough th\n",
            "1 lthough th -> though the\n",
            "2 though the -> hough the \n",
            "3 hough the  -> ough the c\n",
            "4 ough the c -> ugh the co\n",
            "5 ugh the co -> gh the cor\n",
            "6 gh the cor -> h the core\n",
            "7 h the core ->  the core \n",
            "8  the core  -> the core f\n",
            "9 the core f -> he core fu\n",
            "10 he core fu -> e core fun\n",
            "11 e core fun ->  core func\n",
            "12  core func -> core funct\n",
            "13 core funct -> ore functi\n",
            "14 ore functi -> re functio\n",
            "15 re functio -> e function\n",
            "16 e function ->  function \n",
            "17  function  -> function o\n",
            "18 function o -> unction of\n",
            "19 unction of -> nction of \n",
            "20 nction of  -> ction of a\n",
            "21 ction of a -> tion of a \n",
            "22 tion of a  -> ion of a c\n",
            "23 ion of a c -> on of a ch\n",
            "24 on of a ch -> n of a cha\n",
            "25 n of a cha ->  of a chat\n",
            "26  of a chat -> of a chatb\n",
            "27 of a chatb -> f a chatbo\n",
            "28 f a chatbo ->  a chatbot\n",
            "29  a chatbot -> a chatbot \n",
            "30 a chatbot  ->  chatbot i\n",
            "31  chatbot i -> chatbot is\n",
            "32 chatbot is -> hatbot is \n",
            "33 hatbot is  -> atbot is t\n",
            "34 atbot is t -> tbot is to\n",
            "35 tbot is to -> bot is to \n",
            "36 bot is to  -> ot is to m\n",
            "37 ot is to m -> t is to mi\n",
            "38 t is to mi ->  is to mim\n",
            "39  is to mim -> is to mimi\n",
            "40 is to mimi -> s to mimic\n",
            "41 s to mimic ->  to mimic \n",
            "42  to mimic  -> to mimic a\n",
            "43 to mimic a -> o mimic a \n",
            "44 o mimic a  ->  mimic a h\n",
            "45  mimic a h -> mimic a hu\n",
            "46 mimic a hu -> imic a hum\n",
            "47 imic a hum -> mic a huma\n",
            "48 mic a huma -> ic a human\n",
            "49 ic a human -> c a human \n",
            "50 c a human  ->  a human c\n",
            "51  a human c -> a human co\n",
            "52 a human co ->  human con\n",
            "53  human con -> human conv\n",
            "54 human conv -> uman conve\n",
            "55 uman conve -> man conver\n",
            "56 man conver -> an convers\n",
            "57 an convers -> n conversa\n",
            "58 n conversa ->  conversat\n",
            "59  conversat -> conversati\n",
            "60 conversati -> onversatio\n",
            "61 onversatio -> nversation\n",
            "62 nversation -> versationa\n",
            "63 versationa -> ersational\n",
            "64 ersational -> rsationali\n",
            "65 rsationali -> sationalis\n",
            "66 sationalis -> ationalist\n",
            "67 ationalist -> tionalist,\n",
            "68 tionalist, -> ionalist, \n",
            "69 ionalist,  -> onalist, C\n",
            "70 onalist, C -> nalist, Ch\n",
            "71 nalist, Ch -> alist, Cha\n",
            "72 alist, Cha -> list, Chat\n",
            "73 list, Chat -> ist, ChatG\n",
            "74 ist, ChatG -> st, ChatGP\n",
            "75 st, ChatGP -> t, ChatGPT\n",
            "76 t, ChatGPT -> , ChatGPT \n",
            "77 , ChatGPT  ->  ChatGPT i\n",
            "78  ChatGPT i -> ChatGPT is\n",
            "79 ChatGPT is -> hatGPT is \n",
            "80 hatGPT is  -> atGPT is v\n",
            "81 atGPT is v -> tGPT is ve\n",
            "82 tGPT is ve -> GPT is ver\n",
            "83 GPT is ver -> PT is vers\n",
            "84 PT is vers -> T is versa\n",
            "85 T is versa ->  is versat\n",
            "86  is versat -> is versati\n",
            "87 is versati -> s versatil\n",
            "88 s versatil ->  versatile\n",
            "89  versatile -> versatileU\n",
            "90 versatileU -> ersatileUn\n",
            "91 ersatileUn -> rsatileUnl\n",
            "92 rsatileUnl -> satileUnli\n",
            "93 satileUnli -> atileUnlik\n",
            "94 atileUnlik -> tileUnlike\n",
            "95 tileUnlike -> ileUnlike \n",
            "96 ileUnlike  -> leUnlike m\n",
            "97 leUnlike m -> eUnlike mo\n",
            "98 eUnlike mo -> Unlike mos\n",
            "99 Unlike mos -> nlike most\n",
            "100 nlike most -> like most \n",
            "101 like most  -> ike most c\n",
            "102 ike most c -> ke most ch\n",
            "103 ke most ch -> e most cha\n",
            "104 e most cha ->  most chat\n",
            "105  most chat -> most chatb\n",
            "106 most chatb -> ost chatbo\n",
            "107 ost chatbo -> st chatbot\n",
            "108 st chatbot -> t chatbots\n",
            "109 t chatbots ->  chatbots,\n",
            "110  chatbots, -> chatbots, \n",
            "111 chatbots,  -> hatbots, C\n",
            "112 hatbots, C -> atbots, Ch\n",
            "113 atbots, Ch -> tbots, Cha\n",
            "114 tbots, Cha -> bots, Chat\n",
            "115 bots, Chat -> ots, ChatG\n",
            "116 ots, ChatG -> ts, ChatGP\n",
            "117 ts, ChatGP -> s, ChatGPT\n",
            "118 s, ChatGPT -> , ChatGPT \n",
            "119 , ChatGPT  ->  ChatGPT r\n",
            "120  ChatGPT r -> ChatGPT re\n",
            "121 ChatGPT re -> hatGPT rem\n",
            "122 hatGPT rem -> atGPT reme\n",
            "123 atGPT reme -> tGPT remem\n",
            "124 tGPT remem -> GPT rememb\n",
            "125 GPT rememb -> PT remembe\n",
            "126 PT remembe -> T remember\n",
            "127 T remember ->  remembers\n",
            "128  remembers -> remembers \n",
            "129 remembers  -> emembers p\n",
            "130 emembers p -> members pr\n",
            "131 members pr -> embers pre\n",
            "132 embers pre -> mbers prev\n",
            "133 mbers prev -> bers previ\n",
            "134 bers previ -> ers previo\n",
            "135 ers previo -> rs previou\n",
            "136 rs previou -> s previous\n",
            "137 s previous ->  previous \n",
            "138  previous  -> previous p\n",
            "139 previous p -> revious pr\n",
            "140 revious pr -> evious pro\n",
            "141 evious pro -> vious prom\n",
            "142 vious prom -> ious promp\n",
            "143 ious promp -> ous prompt\n",
            "144 ous prompt -> us prompts\n",
            "145 us prompts -> s prompts \n",
            "146 s prompts  ->  prompts g\n",
            "147  prompts g -> prompts gi\n",
            "148 prompts gi -> rompts giv\n",
            "149 rompts giv -> ompts give\n",
            "150 ompts give -> mpts given\n",
            "151 mpts given -> pts given \n",
            "152 pts given  -> ts given t\n",
            "153 ts given t -> s given to\n",
            "154 s given to ->  given to \n",
            "155  given to  -> given to i\n",
            "156 given to i -> iven to it\n",
            "157 iven to it -> ven to it \n",
            "158 ven to it  -> en to it i\n",
            "159 en to it i -> n to it in\n",
            "160 n to it in ->  to it in \n",
            "161  to it in  -> to it in t\n",
            "162 to it in t -> o it in th\n",
            "163 o it in th ->  it in the\n",
            "164  it in the -> it in the \n",
            "165 it in the  -> t in the s\n",
            "166 t in the s ->  in the sa\n",
            "167  in the sa -> in the sam\n",
            "168 in the sam -> n the same\n",
            "169 n the same ->  the same \n",
            "170  the same  -> the same c\n",
            "171 the same c -> he same co\n",
            "172 he same co -> e same con\n",
            "173 e same con ->  same conv\n",
            "174  same conv -> same conve\n",
            "175 same conve -> ame conver\n",
            "176 ame conver -> me convers\n",
            "177 me convers -> e conversa\n",
            "178 e conversa ->  conversat\n",
            "179  conversat -> conversati\n",
            "180 conversati -> onversatio\n",
            "181 onversatio -> nversatioC\n",
            "182 nversatioC -> versatioCh\n",
            "183 versatioCh -> ersatioCha\n",
            "184 ersatioCha -> rsatioChat\n",
            "185 rsatioChat -> satioChatG\n",
            "186 satioChatG -> atioChatGP\n",
            "187 atioChatGP -> tioChatGPT\n",
            "188 tioChatGPT -> ioChatGPT \n",
            "189 ioChatGPT  -> oChatGPT s\n",
            "190 oChatGPT s -> ChatGPT su\n",
            "191 ChatGPT su -> hatGPT suf\n",
            "192 hatGPT suf -> atGPT suff\n",
            "193 atGPT suff -> tGPT suffe\n",
            "194 tGPT suffe -> GPT suffer\n",
            "195 GPT suffer -> PT suffers\n",
            "196 PT suffers -> T suffers \n",
            "197 T suffers  ->  suffers f\n",
            "198  suffers f -> suffers fr\n",
            "199 suffers fr -> uffers fro\n",
            "200 uffers fro -> ffers from\n",
            "201 ffers from -> fers from \n",
            "202 fers from  -> ers from m\n",
            "203 ers from m -> rs from mu\n",
            "204 rs from mu -> s from mul\n",
            "205 s from mul ->  from mult\n",
            "206  from mult -> from multi\n",
            "207 from multi -> rom multip\n",
            "208 rom multip -> om multipl\n",
            "209 om multipl -> m multiple\n",
            "210 m multiple ->  multiple \n",
            "211  multiple  -> multiple l\n",
            "212 multiple l -> ultiple li\n",
            "213 ultiple li -> ltiple lim\n",
            "214 ltiple lim -> tiple limi\n",
            "215 tiple limi -> iple limit\n",
            "216 iple limit -> ple limita\n",
            "217 ple limita -> le limitat\n",
            "218 le limitat -> e limitati\n",
            "219 e limitati ->  limitatio\n",
            "220  limitatio -> limitation\n",
            "221 limitation -> imitations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVFlILiOixdc",
        "outputId": "686771c3-5245-47aa-cb12-912b0d34bbce"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13, 7, 4, 18, 10, 14, 24, 18, 2, 4]\n",
            "[7, 4, 18, 10, 14, 24, 18, 2, 4, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
        "\n",
        "## 문제(4) : x_data를 원핫인코딩 > numpy의 eye를 쓸 수 있지 않을까?\n",
        "x_one_hot = np.eye(np.max(x_data) + 1)[x_data]\n",
        "\n",
        "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
        "\n",
        "## 문제(5) : x_one_hot과 y_data 텐서로 변환 : 둘 다 같은 형식의 텐서로 변환하면 될까?? (FloatTensor, LongTesor 중 맞는 것은?)\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "5lPes1dvjlNb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##8. 크기 확인\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMZzZlaymMk8",
        "outputId": "a42c2595-3a64-4620-c021-118150206519"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([222, 10, 27])\n",
            "레이블의 크기 : torch.Size([222, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##9.원핫인코딩 결과 샘플 확인하기\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knx1DE_AmSFB",
        "outputId": "ba9c7c96-87e2-4e36-ab9c-dcaf4c5980ee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##10. 레이블 데이터 샘플 확인하기\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pWDiH1SmYT_",
        "outputId": "3fbcb3f7-88ab-44a6-c6ec-a003296fa951"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 7,  4, 18, 10, 14, 24, 18,  2,  4, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##11. RNN 모델 구현\n",
        "\n",
        "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
        "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
        "### batch_fisrt 설정 필요할까? (유튜브 강의 참고)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, layers, batch_first = True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "-Ww22xu8mfUc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 2)"
      ],
      "metadata": {
        "id": "No2GRvTpnLBl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##12. loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "##13. optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "9-zuJLeUnQLB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##14. 출력 크기 점검\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-RxRaiHnh9U",
        "outputId": "394f7111-ddc5-4641-9f1c-b1f50337245e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([222, 10, 27])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##15. Training 시작\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    ##문제(7) : outputs, Y 형태 그대로 넣으면 안되죠. view 함수를 이용해 loss값을 계산해봅시다.\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #16. 예측결과 확인\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
        "            predict_str += ''.join([world_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += world_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxxrxCd2nwoo",
        "outputId": "cf4e374a-0742-4e90-e150-ccc120ba1ae5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lluuuubbuuuuuuubuubupbuuubuubuuubuuuubbuuuuuuuuuuuuuuuuuuuuububuumuuuuububuuuuuuuuuuubuuuuuumuuuububuuuuumuubuubuuuubbuuuuuuuuubumuuuumuuuuuumuulubuuuluuuuubuuumuuuuuuuuuuuuuuuuuumuubuumuuuuuubuuuubububbubuubuluuuubuuubuuuuuuuubuub\n",
            "lrrllrlrerrrerrolmrrolrolrelrererrelolrorreroerrrrreemrrorrerrrrroreroroerrrooorrelloreerereorerorrrrrreemrrrooorelolrooerrelloreeerrrrrelererrolrrerelrrrrorerererlorrorrerrrerermmrrrrrorerorlrellorererrrorerelrerrrrorrrlrrrrrororo\n",
            "lrr r rrer rerrerererrror er eeereeror eerereeeeerreeererrrerere ereror errreeeererre eerereererorrrrreeeeerrrereeror  eeeeerre eeerrrrrerererrer rereerrrrerer rereeereeeererererrerrre ererorererre erererererer errrrorrrrrrrereror \n",
            "lrree reeeeeeeee eee eeee ee eeeeeee e eeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeee ee eeeeeeeeeeee  eeeeeeeeeeeeee  e  eeeee ee eeeeeeeeeeeeeeeeee eeeeeee eeeee eeeeeeeeeeeeeeeee eeeee eeeeeeeee ee eeeee eeeeeeeeeeeree  eeeeeee eee\n",
            "i  ee eeeeeeeeee eeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeee ee eeeeeeeeeeee  eeeeeeeeeeeeeeeeeeeeeeee ee eeeeeeeeeeeeeeeeee eeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeee eeeeeeeee ee eeeee eeeeeeeeeeeeeee eeeeeeeeeee\n",
            "i                         ee e          e     ee           ee  e     ee       ee       e  e     e      e  e   ee        ee       e          e  e     e      e e  e   e  e e         e  e     ee                 e   e               ee \n",
            "i                                                                                                                                                                                                                                      \n",
            "                                                                                                                                                                                                                                       \n",
            "                                                                                                                                                                                                                                       \n",
            "                                                                                                                                                                                                                                       \n",
            "                     t       t      o   o                  o         o             o            o         o   oo   o         o                 o                 o      o           o        o     o                                   \n",
            "             o    o  t o  t  t  o   o   o                t oo  t s   o  tio   o  tio   t  o    io    o    o   oo t o    o  tio             i   o   ti            o   o  o           o  t s   o   t o            o  ti   oo   i         \n",
            "        ti   o    o  t o  t  ti o  ioo to  i    ttoo     t oo  t s  io  tio   o  tioo  t to s  io   too   o  too tioo   o  tioo  t    o    i   o   tis  to    t  o  to  o           o  t s  io   tioo       s tio  ti   oo  ti    oos  \n",
            "t     t ti   t    o  tioo ti ti o tioo to ti o  ttoo    tt oo  t s tio  tio   o  tioo  t ti s tio   tio   o  too tioo   oo tioo  t ti o    is tio  tis  to    t  o  to to ti     i  o  t s tio   tioo    to s tio  ti  too  tio  ioos  \n",
            "t     t ti   o  t o  tion ti ti o tioo to ti o  ttio    tt oo  t s tion tio  oo  tioo  t ti s tioo  tio   i  too tioo   oo tioo  t ti o  ttis ti   tis  ti o  t  o  to to ti     i  o  t sttion  tioo    to s tio  ti ttoo  ti   iooo  \n",
            "tt    t ti   o  t o  tion ti ti o tioo to ti oi ttto  o tt oo  t sttion tist oo  tioo  o ti s tioo  tio   i  too tioo   oi tioo  o ti o st is to   tis tti o  t  t  to to ti t i i  o  i sttion  tioo    ti s tio  ti ttoo  ti t  too  \n",
            "tt    t ti   o  t o  tion tn ti i tioo oo ti ti tttti o tt oo  i sttion tist oo  tioT so ti sttioo  tio   i  ioo tion   oi tioT se ti t sttis to   tis tti oost  t  to to ti t i i  o  i sttion  tioT s  ti s tio  ti ttoo  ti t  tion \n",
            "tt    t ti   o    i  tion t  t  i tion oo ti ti tttti o tt oo  i sttion t st oo  tioT oo ti sttioe  tio  si  ioo tion   o  tioT oe tt t sttis to   tis ti  iost  t  to to ti t i i mi  i sttion  tioT o  ti s tis  ti ttoi  ti tt tion \n",
            "tt o  o ti   o    i  tion t  t  i tion oo ti ti ttttt o tt oo  i sttion t st oo  tioT oo ti sttioe  tio  si tioo tion   o  tioT oe tt t sttis to   tis ti  iost  ti to oo ti t i i mi  i sttion  tioT o  ti s tis tti ttoi  tistt tion \n",
            "tt o  o tis  o    i  tion t  t  o tion oo ti ti ttttt o tt oo  i sttion t st oo  tiPT oo ti sttioe  tto  si ttoo tion   o  tiPT oe tttt sttis to   tis tit tist  ti to oo ti t i i mt  i sttion  tiPT o  tt s tio tti ttoo  tisttition \n",
            "tt o so tos so s ti  tion t  t  o tion oo ti ti ttttt i ttioo  i sttion tist oo  tiPT so ti sttioe  ttoe si ttio tion   o  tiPT se tttt sttis to   tis tit tist  ti te oo ti tpi i tt  t s tion  tiPT s  tt s tio tti ttooe tisttition \n",
            "tt o soatoo po s ti  tion t  t  o tion oo ti te tttttte ttioo  t sation tist oo  tiPT so ti satioe  ttoe si ttio tion   oo tiPT se tttt s tis to   tio tit tist  ti te oe ti tpiti to  t sationsatiPT s  tt satio tti ttooe tisttition \n",
            "ttoo soatoo po s tin tion tr t io tion eo ti te tstttte ttioo  t sation tisttioe tiPT so ti satioe  ttoe se ttio tion   ooatiPT se ttto satio to satio tit tist  ti se oe ti tciti to  t sationsatiPT    te satio tti ttooe tiststtion \n",
            "ttoo soatoo co s tin tion tr t ioation eo ti te tstttte ttioo  t sation tisttiohatiPT  s to satioe  ttoe  e  tioation   ooatGPT  e t to satio to satioattt ti t  ti se oo th tceti to  t sationsatiPT    te satio ttiattooe tiststtion \n",
            "ttio soatoo co s fin tion tr e ioation eo th te tstttte tt oo  e sation tistt ohatGPT  s to satioe  tioe  e  tioation   ooatGPT  s s to satio to s tioattt ti e  ti se oo th tceti to se sationsatiPT    te satio stiattooe ti tsttion \n",
            "ttio soatoo co s fon tion tr e toatGon eo th te tstt te tt oo  e sation tistt ohatGPT  s to satioe  tioe  e  tioatGon   ooatGPT  s s to s tioato s tioat t time  ti te oo th  ceti to  e sationsatGPT    te s tio stiattooe timtottron \n",
            "ttbo poatio co s fin tion tr e toatGonseo th te tstt te tt co  e sation tistt ohatGPT  s to satioe  tioe  e  tcoatGons  ohatGPT  s s fo s tioato s tioat i time  ti to oo th  ceti to  e sationsatGPT    te s tio stiattooe cimtottron \n",
            "ttbo poatio co s fen tion tr e toatGonseo ti temtott te tt co  i sation tistt ohatGPT  s te satioe  tioe  e  tcoatGons  ohatGPT  s s fo s tioato s tioat i time  th to oo th  ceti co  e sationsatGPT    te s tio stiattooe cimtottron \n",
            "ttbonpo tio co s cen tion tr e toatGonseo ti temtott te tt co  i s tion tistt ohatGPT  s te satioe  tioe  e  tcoatGons  ohatGPT  s s fo s tioaio s tioat s timer th to oh th  cemi co  e s tionsatGPT    te s tio stiattote cimtottron \n",
            "ttbonpo tio co s cen tion tr e thatGonseh thntimtott ie tt co  ers tion tistt ohatGPT  s te satioe  tioe  e  icoatGons  ohatGPT  s s co s tioaio s tioat s timer th to eh th  cemi co  ers tionsatGPT    te s tio stiattote cimtott on \n",
            "ttbonpo tio co s cenmtion tr e ihatGonsih thnmimtott iemtt co  ers tion tistG ohatGPT  s te satioe  tioe  e aicoatGons  ohatGPT  s s co s tioaio s pioaite timer th to eh th  cemi co  ers tionsatGPT s  te s tie stiattote cimtott on \n",
            "ttbonbo the co s cenmtion tr e ihatGon ih tinmimtott iemtt co  ersatioe tistG ohatGPT rs te satioe  tioe  e aicoatGon   ohatGPT rs s io s tioaio s pioaite timer th to eh th  cemi co  ers tioehatGPT r mte s tiemetiatioto cimtott on \n",
            "ttbonbo the co s cenmtion ir e ihatGon ih tinmimtott iemtt co  ersatioe tistG PhatGPT rs te satioe  vioe  e aicoatGon   ohatGPT rs s io s tiomio s pioaiti pimer th to eh th  cemi conversatioehatGPT r mte s tiemetiatioto cimto t on \n",
            "ttGonbo the co s cenmtion ir e ihatGon ih thnmimiott iemat co  ersatioe tistG PhatGPT rs te satioe  vioe  e  icoatGon   ohatGPT rs s io s piomio s pioaiti pimer th to eh th  cemi conversatioehatGPT r mte s tiemetimtilte cimto t on \n",
            "ttGonbo the co s fenmtion ir e ihatGon is thnmimiott iemat co  ersatioe tistG ChatGPT rs te satioe  vioe  em icoatGon   ohatGPT rs smio s piomio s piomits pimer th to eh th  cemi conversatioehatGPT r mte s tiemetimtilte cimto t on \n",
            "ttGonbi the co e fenmtion ir e ihatGon is tinmimiott iemat co versatioe tistG ChatGPT is pe satioe  tioe  im icoatGon   ohatGPT is smfo s piomio s piomtts pimer th te oh th  cemi conversatioehatGPT i mce s tiemetimtilte cimto t on \n",
            "ttGonpi the co e fenmtion ir e ihatGot is tincimiott iemat co versatioe tistG ChatGPT is pe satioe  pioe  im ichatGot   ihatGPT is s fo s piomio s piomtts pimer th te oh th  cemi conversatioehatGPT i mce s piem trmtilte cimto t on \n",
            "ttGonpi the co e funmtion ir e ihatGot is tincimioat iemat co versation tistG ChatGPT is persatioe  pioe  im  chatGot   ihatGPT is s fo s piomio s piomtts pimer th te oh the cemi conversationhatGPT i mce s prem trmtilte cimto t on \n",
            "ttGonpi the co e funmtion ir e chatGot is tinmimilat iemat co versation tistG ChatGPT is persatioe nvile  i   chatGot   ChatGPT is s fo s piomio s piomtts pimer th te on the cemi conversationhatGPT i mce s prem trmtilte cimto t on \n",
            "ttGonpi the co e funmtion ir e chatbot is tinmimilat iumat co versation tistG ChatGPT rs versatioe nvile  i   chatGot   ChatGPT rs s fo s promio s piomtts piver th te on the cemi conversationhatGPT r fce s prem trmtilte cimio tGon \n",
            "ttGonpi the co e funmtion ir e chatbot is tinmimilat iumat co versation tistG ChatGPT ss versatioe nvile  o   chatGot   ChatGPT ss smfo s premions piomtts piver th te on the semi conversationhatGPT s fce s trem t mtilte cimio tGon \n",
            "ttGonpi the co e funmtion ir e chatGotsis tinmimilat iumat co versation tistG ChatGPT ss versatiot nvile fo   chatGot   ChatGPT se smfo s premions piomtts piver th ti en the semi conversationhatGPT s fce s tremet mtilte cimit tGon \n",
            "ttGonpi the co e funmtion ih e chatGotsis ti mimilat iumat co versation tistG ChatGPT ss versatiot nvile fo   chatGots  ChatGPT se imfo s premions piomtts fiver th ti en the semi conversationhatGPT s fcers tremetumtilte cimit tGon \n",
            "ttGonpi the co e funmtion ih e chatGotsis ti mimilat humat co versation tistG ChatGPT ss versatioe nvike fom  chatGots  ChatGPT se imfo s premiors promtts fiver ti ti en the semi conversationhatGPT s fcers prememumtilte cimit tGon \n",
            "ttGonpi the co e funmtion ih e chatGotsis ti mimilat humat conversation tistG ChatGPT ss versatioe nvike fom  chatGots  ChatGPT se imfors premiors promtts fiver ti tt on the semi conversationhatGPT s fcers frememumtilte cimit tGon \n",
            "ttGonpi the co e funmtion ih e chatGotsis ti mimilat humat conversation tistG ChatGPT ss versatioe nvike fim  chatGots  ChatGPT se emfors premiors promtts piver ti tt on the semi conversationhatGPT s fcers frememumtilte cimit tGon \n",
            "ttGonpi the co e funmtion ih e chatGotsis ti mimilat humatfconversation tistG ChatGPT rs versatioe nvike fim  chatGot   ChatGPT re emfors premiors prompts piver th tt on the semi conversationhatGPT r fcers frememumtilte cimit tGon \n",
            "ttGonpi the co e funmtion ih e chatGotsis to mimioat humatfconversation tist, ChatGPT rs versatioe nvike fim  chatGot   ChatGPT re emfors premiors prompts piver th ta in the semi conversationhatGPT r fcers frem mumtilte cimit tGon \n",
            "tthonpo the co e funmtion ih e chatbotsis to mimio t humatfconversation tist, ChatGPT rs versatioe nvike mim  chatGot   ChatGPT re emfors premiors prompts piven th ta in the semi conversationhatGPT r fcers frem mrmtilte cimit tGon \n",
            "lthonpo the co e funmtion ih e chatbotsis to mimio t humatfconversationatist, ChatGPT rs versatioe ntike mim  chatbots  ChatGPT rememfors premiors prompts given th tt in the semi conversationhatGPT r fcers frem mrmtilte cimit tGon \n",
            "lthonpo the co e funmtion ir e chatbotsis to mimio t humatfconversationatist, ChatGPT rs versatioe ntike mim  chatbots  ChatGPT rememfors premiors prompts given ti tt in the seme conversationhatGPT r ffers from mrmtille cimit tion \n",
            "lthonpo the co e funmtion or e chatbotsis to mimio t humatfconversationatist, ChatGPT rs versatioe ntike mimt chatbots  ChatGPT rememfers premiors prompts given ti tt in the seme conversationhatGPT r ffers from mrmtiple cimit tion \n",
            "lthotpo the co e funmtion or e chatbotsis to mimio a humatfconversationatist, ChatGPT rs versatioe nlike mimt chatbots  ChatGPT rememfers premiors prompts given ti tt in the seme conversationhatGPT ruffers from mrmtiple cimit tion \n",
            "lthotpo the co e funmtion or e chatbotsis to mimio a humat conversationatist, ChatGPT rs versatioe nlike mimt chatbots  ChatGPT remembers premiors prompts given ti tt in the seme conversationhatGPT ruffers from mrmtiple cimit tion \n",
            "lthotpo the co e funmtion oh e chatbotsis to mimis a humat conversationatist, ChatGPT rs versatioe nlike mimt chatbots  ChatGPT remembers premiors prompts given to tt in the seme conversationhatGPT ruffers from mrmtiple cimit tion \n",
            "lthotpo the co e funmtion oh t chatbotsis to mimis a humat conversationatist, ChatGPT rs versatioe nlike mimt chatbots  ChatGPT remembers premiors prompts given to tt in the seme conversationhatGPT ruffers from mrmtiple cimit tion \n",
            "lthotpo the co e funmtion oh t chatbotsis to mimis a human conversationatist, ChatGPT rs versatioe nlike mimt chatbots  ChatGPT remembers premiors prompts given to tt in the seme conversationhatGPT ruffers from mrmtiple cimit tion \n",
            "lthotpo the co e funmtion oh a chatbotsis to mimis a human conversationatist, ChatGPT rs versatioe nlike momt chatbots  ChatGPT remembers premiors prompts given to tt in the seme conversationhatGPT ruffers from mumtiple cimit tion \n",
            "lthotpo the co e function oh a chatbotsis to mimis a human conversationatist, ChatGPT rs versatioeUnlike momt chatbots  ChatGPT remembers premiors prompts given to it in the seme conversationhatGPT ruffers from multiple cimit tion \n",
            "lthotpo the co e function oh a chatbotsis to mimis a human conversationatist, ChatGPT rs versatioeUnlike momt chatbots  ChatGPT remembers premiors prompts given to it in the same conversationhatGPT ruffers from multiple cimit tion \n",
            "ltho ph the co e function of a chatbotsis to mimic a human conversationatist, ChatGPT rs versatioeUnlike momt chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple timit tion \n",
            "ltho ph the co e function of a chatbot is to mimic a human conversationatist, ChatGPT rs versatioeUnlike momt chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple timit tion \n",
            "ltho ph the co e function of a chatbotsis to mimic a human conversationatist, ChatGPT rs versatioeUnlike momt chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple timit tion \n",
            "ltho ph the co e function of a chatbot is to mimic a human conversationatist, ChatGPT rs versatioeUnlike momt chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple timit tion \n",
            "ltho gh the co e function of a chatbot is to mimic a human conversationatist, ChatGPT rs versatioeUnlike momt chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple limit tion \n",
            "ltho gh the co e function of a chatbot is to mimic a human conversationatist, ChatGPT rs versatioeUnlike momt chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple limit tion \n",
            "ltho gh the co e function of a chatbotsis to mimic a human conversationalist, ChatGPT rs versatioeUnlike momt chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple limit tion \n",
            "ltho gh the co e function of a chatbotsis to mimic a human conversationalist, ChatGPT rs versatioeUnlike momt chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple limit tiona\n",
            "ltho gh the co e function of a chatbotsis to mimic a human conversationalist, ChatGPT rs versatioeUnlike momt chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple limit tiona\n",
            "ltho gh the co e function of a chatbotsis to mimic a human conversationalist, ChatGPT rs versatioeUnlike most chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple limit tiona\n",
            "ltho gh the co e function of a chatbot is to mimic a human conversationalist, ChatGPT rs versatioeUnlike most chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple limit tiona\n",
            "ltho gh the co e function of a chatbot is to mimic a human conversationalist, ChatGPT rs versatioeUnlike most chatbots, ChatGPT remembers premious prompts given to it in the same conversationhatGPT ruffers from multiple limit tiona\n",
            "ltho gh the co e function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers premious prompts given to it in the same conversationhatGPT ruffers from multiple limit tiona\n",
            "ltho gh the co e function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers premious prompts given to it in the same conversationhatGPT suffers from multiple limit tiona\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers premious prompts given to it in the same conversationhatGPT suffers from multiple limit tiona\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers premious prompts given to it in the same conversationhatGPT suffers from multiple limit tiona\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers premious prompts given to it in the same conversationhatGPT suffers from multiple limit tiona\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT rs versatioeUnlike most chatbots, ChatGPT remembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "ltho gh the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatioeUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n",
            "lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "8qUkbiw2t0Il",
        "outputId": "df2469f9-b4e3-4cad-bacb-4ad35f57ab75"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lthough the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatileUnlike most chatbots, ChatGPT iemembers previous prompts given to it in the same conversationhatGPT suffers from multiple limitationa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과가 어떤가요?? 마지막 에폭의 문장이 그럴싸한가요?"
      ],
      "metadata": {
        "id": "PkIzDTdyvTHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task2\n",
        "\n",
        "위 sentence는 제가 임의로 생성한 문장들입니다.\n",
        "\n",
        "마음에 드시는 문구 가져오셔서 문장이 어떻게 생성되는지 확인해보세요! \n",
        "\n",
        "영어가 아닌 한국어로 시도해보는 것도 좋겠죠? \n",
        "\n",
        "수정이 많이 필요(토큰화 등) 할 수 있으나 한번 시도해보시는 것 권장드립니다 :)\n",
        "\n",
        "위 베이스라인은 어디든 수정하셔도 좋고 조금 더 자연스러운 문장이 나올 수 있게 다양한 시도를 해보세요!\n",
        "\n",
        "조건 : 문장 3개 이상, 연결성이 있는 문장을 \" \" 으로 구분하여 ( )에 넣기"
      ],
      "metadata": {
        "id": "kN1zL8Dpvane"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 생성할 문장 데이터\n",
        "\n",
        "sentence = (\"Although the core function of a chatbot is to mimic a human conversationalist, ChatGPT is versatile\"\n",
        "            \"Unlike most chatbots, ChatGPT remembers previous prompts given to it in the same conversatio\"\n",
        "            \"ChatGPT suffers from multiple limitations\")"
      ],
      "metadata": {
        "id": "IKp-lKrjvXR9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCzP1yjfjTGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}