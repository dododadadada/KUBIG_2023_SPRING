{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmQ5F7UAeKB_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task1\n",
        "\n",
        "빈 칸을 채워주세요!\n",
        "\n",
        "단계별 output이 github 파일에는 남아있으니 그 output과 동일한 형태인지 확인하면서 진행해주시면 됩니다~"
      ],
      "metadata": {
        "id": "Sgxd6SxmeVcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 생성할 문장 데이터\n",
        "\n",
        "sentence = (\"Brick walls are there for a reason and you must not think \"\n",
        "            \"that the brick walls aren't there to keep us out, but rather \"\n",
        "            \"in this way that the brick walls are there to show us how badly we want things.\")"
      ],
      "metadata": {
        "id": "NDvUeC8BoUb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. 문자 집합 만들기\n",
        "world_set = list(set(sentence))\n",
        "\n",
        "## 문제(1): 각 문자에 정수 인코딩 (공백도 하나의 원소로 포함)\n",
        "vocab = {c: i for i, c in enumerate(world_set)}"
      ],
      "metadata": {
        "id": "b9lkrKyZf8ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_0we5Y-gYDq",
        "outputId": "b7bd31ad-7d19-43a4-d166-76bb8487e9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'s': 0, 'a': 1, 'r': 2, 'p': 3, 'B': 4, 'b': 5, \"'\": 6, 't': 7, ',': 8, 'd': 9, 'c': 10, 'y': 11, ' ': 12, 'h': 13, 'g': 14, 'l': 15, 'o': 16, 'k': 17, 'u': 18, 'n': 19, 'm': 20, 'w': 21, 'f': 22, '.': 23, 'e': 24, 'i': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. 문자 집합 크기 확인\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('문자 집합 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpKupU6lgpfT",
        "outputId": "b2fef61a-d299-438f-d901-c7935496806f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 크기 : 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
        "\n",
        "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
        "sequence_length = 10  # 너무 길거나 너무 짧게 잡으면 안됩니다!\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "wFDZJHSMg9In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. seqence 길이 단위 자르기\n",
        "\n",
        "# 데이터 구성을 위한 리스트\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "## 문제(2): 반복문 내에서의 인덱싱을 사용하여 sequence_length 값 단위로 샘플을 잘라 데이터 만들기, y_str은 x_str은 한 칸씩 쉬프트된 sequnce\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "  x_str = sentence[i:i+10]\n",
        "  y_str = sentence[i+1:i+11]\n",
        "  print(i, x_str, \"->\", y_str)\n",
        "\n",
        "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
        "  x_data.append([vocab[c] for c in x_str])\n",
        "  y_data.append([vocab[d] for d in y_str])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_lBuSBiT1P8",
        "outputId": "562a3b5f-7f57-494b-91c7-0f4542ccf5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Brick wall -> rick walls\n",
            "1 rick walls -> ick walls \n",
            "2 ick walls  -> ck walls a\n",
            "3 ck walls a -> k walls ar\n",
            "4 k walls ar ->  walls are\n",
            "5  walls are -> walls are \n",
            "6 walls are  -> alls are t\n",
            "7 alls are t -> lls are th\n",
            "8 lls are th -> ls are the\n",
            "9 ls are the -> s are ther\n",
            "10 s are ther ->  are there\n",
            "11  are there -> are there \n",
            "12 are there  -> re there f\n",
            "13 re there f -> e there fo\n",
            "14 e there fo ->  there for\n",
            "15  there for -> there for \n",
            "16 there for  -> here for a\n",
            "17 here for a -> ere for a \n",
            "18 ere for a  -> re for a r\n",
            "19 re for a r -> e for a re\n",
            "20 e for a re ->  for a rea\n",
            "21  for a rea -> for a reas\n",
            "22 for a reas -> or a reaso\n",
            "23 or a reaso -> r a reason\n",
            "24 r a reason ->  a reason \n",
            "25  a reason  -> a reason a\n",
            "26 a reason a ->  reason an\n",
            "27  reason an -> reason and\n",
            "28 reason and -> eason and \n",
            "29 eason and  -> ason and y\n",
            "30 ason and y -> son and yo\n",
            "31 son and yo -> on and you\n",
            "32 on and you -> n and you \n",
            "33 n and you  ->  and you m\n",
            "34  and you m -> and you mu\n",
            "35 and you mu -> nd you mus\n",
            "36 nd you mus -> d you must\n",
            "37 d you must ->  you must \n",
            "38  you must  -> you must n\n",
            "39 you must n -> ou must no\n",
            "40 ou must no -> u must not\n",
            "41 u must not ->  must not \n",
            "42  must not  -> must not t\n",
            "43 must not t -> ust not th\n",
            "44 ust not th -> st not thi\n",
            "45 st not thi -> t not thin\n",
            "46 t not thin ->  not think\n",
            "47  not think -> not think \n",
            "48 not think  -> ot think t\n",
            "49 ot think t -> t think th\n",
            "50 t think th ->  think tha\n",
            "51  think tha -> think that\n",
            "52 think that -> hink that \n",
            "53 hink that  -> ink that t\n",
            "54 ink that t -> nk that th\n",
            "55 nk that th -> k that the\n",
            "56 k that the ->  that the \n",
            "57  that the  -> that the b\n",
            "58 that the b -> hat the br\n",
            "59 hat the br -> at the bri\n",
            "60 at the bri -> t the bric\n",
            "61 t the bric ->  the brick\n",
            "62  the brick -> the brick \n",
            "63 the brick  -> he brick w\n",
            "64 he brick w -> e brick wa\n",
            "65 e brick wa ->  brick wal\n",
            "66  brick wal -> brick wall\n",
            "67 brick wall -> rick walls\n",
            "68 rick walls -> ick walls \n",
            "69 ick walls  -> ck walls a\n",
            "70 ck walls a -> k walls ar\n",
            "71 k walls ar ->  walls are\n",
            "72  walls are -> walls aren\n",
            "73 walls aren -> alls aren'\n",
            "74 alls aren' -> lls aren't\n",
            "75 lls aren't -> ls aren't \n",
            "76 ls aren't  -> s aren't t\n",
            "77 s aren't t ->  aren't th\n",
            "78  aren't th -> aren't the\n",
            "79 aren't the -> ren't ther\n",
            "80 ren't ther -> en't there\n",
            "81 en't there -> n't there \n",
            "82 n't there  -> 't there t\n",
            "83 't there t -> t there to\n",
            "84 t there to ->  there to \n",
            "85  there to  -> there to k\n",
            "86 there to k -> here to ke\n",
            "87 here to ke -> ere to kee\n",
            "88 ere to kee -> re to keep\n",
            "89 re to keep -> e to keep \n",
            "90 e to keep  ->  to keep u\n",
            "91  to keep u -> to keep us\n",
            "92 to keep us -> o keep us \n",
            "93 o keep us  ->  keep us o\n",
            "94  keep us o -> keep us ou\n",
            "95 keep us ou -> eep us out\n",
            "96 eep us out -> ep us out,\n",
            "97 ep us out, -> p us out, \n",
            "98 p us out,  ->  us out, b\n",
            "99  us out, b -> us out, bu\n",
            "100 us out, bu -> s out, but\n",
            "101 s out, but ->  out, but \n",
            "102  out, but  -> out, but r\n",
            "103 out, but r -> ut, but ra\n",
            "104 ut, but ra -> t, but rat\n",
            "105 t, but rat -> , but rath\n",
            "106 , but rath ->  but rathe\n",
            "107  but rathe -> but rather\n",
            "108 but rather -> ut rather \n",
            "109 ut rather  -> t rather i\n",
            "110 t rather i ->  rather in\n",
            "111  rather in -> rather in \n",
            "112 rather in  -> ather in t\n",
            "113 ather in t -> ther in th\n",
            "114 ther in th -> her in thi\n",
            "115 her in thi -> er in this\n",
            "116 er in this -> r in this \n",
            "117 r in this  ->  in this w\n",
            "118  in this w -> in this wa\n",
            "119 in this wa -> n this way\n",
            "120 n this way ->  this way \n",
            "121  this way  -> this way t\n",
            "122 this way t -> his way th\n",
            "123 his way th -> is way tha\n",
            "124 is way tha -> s way that\n",
            "125 s way that ->  way that \n",
            "126  way that  -> way that t\n",
            "127 way that t -> ay that th\n",
            "128 ay that th -> y that the\n",
            "129 y that the ->  that the \n",
            "130  that the  -> that the b\n",
            "131 that the b -> hat the br\n",
            "132 hat the br -> at the bri\n",
            "133 at the bri -> t the bric\n",
            "134 t the bric ->  the brick\n",
            "135  the brick -> the brick \n",
            "136 the brick  -> he brick w\n",
            "137 he brick w -> e brick wa\n",
            "138 e brick wa ->  brick wal\n",
            "139  brick wal -> brick wall\n",
            "140 brick wall -> rick walls\n",
            "141 rick walls -> ick walls \n",
            "142 ick walls  -> ck walls a\n",
            "143 ck walls a -> k walls ar\n",
            "144 k walls ar ->  walls are\n",
            "145  walls are -> walls are \n",
            "146 walls are  -> alls are t\n",
            "147 alls are t -> lls are th\n",
            "148 lls are th -> ls are the\n",
            "149 ls are the -> s are ther\n",
            "150 s are ther ->  are there\n",
            "151  are there -> are there \n",
            "152 are there  -> re there t\n",
            "153 re there t -> e there to\n",
            "154 e there to ->  there to \n",
            "155  there to  -> there to s\n",
            "156 there to s -> here to sh\n",
            "157 here to sh -> ere to sho\n",
            "158 ere to sho -> re to show\n",
            "159 re to show -> e to show \n",
            "160 e to show  ->  to show u\n",
            "161  to show u -> to show us\n",
            "162 to show us -> o show us \n",
            "163 o show us  ->  show us h\n",
            "164  show us h -> show us ho\n",
            "165 show us ho -> how us how\n",
            "166 how us how -> ow us how \n",
            "167 ow us how  -> w us how b\n",
            "168 w us how b ->  us how ba\n",
            "169  us how ba -> us how bad\n",
            "170 us how bad -> s how badl\n",
            "171 s how badl ->  how badly\n",
            "172  how badly -> how badly \n",
            "173 how badly  -> ow badly w\n",
            "174 ow badly w -> w badly we\n",
            "175 w badly we ->  badly we \n",
            "176  badly we  -> badly we w\n",
            "177 badly we w -> adly we wa\n",
            "178 adly we wa -> dly we wan\n",
            "179 dly we wan -> ly we want\n",
            "180 ly we want -> y we want \n",
            "181 y we want  ->  we want t\n",
            "182  we want t -> we want th\n",
            "183 we want th -> e want thi\n",
            "184 e want thi ->  want thin\n",
            "185  want thin -> want thing\n",
            "186 want thing -> ant things\n",
            "187 ant things -> nt things.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVFlILiOixdc",
        "outputId": "da1350f3-f9cc-4456-a55d-ff3fa8ce4af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 2, 25, 10, 17, 12, 21, 1, 15, 15]\n",
            "[2, 25, 10, 17, 12, 21, 1, 15, 15, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
        "\n",
        "## 문제(4) : x_data를 원핫인코딩 > numpy의 eye를 쓸 수 있지 않을까?\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "\n",
        "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
        "\n",
        "## 문제(5) : x_one_hot과 y_data 텐서로 변환 : 둘 다 같은 형식의 텐서로 변환하면 될까?? (FloatTensor, LongTesor 중 맞는 것은?)\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "5lPes1dvjlNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##8. 크기 확인\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMZzZlaymMk8",
        "outputId": "dbd7e8d0-4cff-47d4-abbe-ef76f722b8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([188, 10, 26])\n",
            "레이블의 크기 : torch.Size([188, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##9.원핫인코딩 결과 샘플 확인하기\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knx1DE_AmSFB",
        "outputId": "9b7d1434-a4c8-445d-8f82-ba28877dfc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##10. 레이블 데이터 샘플 확인하기\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pWDiH1SmYT_",
        "outputId": "29c51ad7-8698-4a6d-ca1d-42afc160accc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2, 25, 10, 17, 12, 21,  1, 15, 15,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##11. RNN 모델 구현\n",
        "\n",
        "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
        "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
        "### batch_fisrt 설정 필요할까? (유튜브 강의 참고)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers = layers, batch_first = True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "-Ww22xu8mfUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 2)"
      ],
      "metadata": {
        "id": "No2GRvTpnLBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##12. loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "##13. optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "9-zuJLeUnQLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##14. 출력 크기 점검\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-RxRaiHnh9U",
        "outputId": "2f4a345b-15ba-4417-f39d-37ea1fba663f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([188, 10, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##15. Training 시작\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    ##문제(7) : outputs, Y 형태 그대로 넣으면 안되죠. view 함수를 이용해 loss값을 계산해봅시다.\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #16. 예측결과 확인\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
        "            predict_str += ''.join([world_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += world_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxxrxCd2nwoo",
        "outputId": "659cf411-653d-4beb-8d85-14989209d935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dhdhdhhhhhhhhdhhhhhdhhhdhhhdhhhdhhhhhdhhhdhhdhdhhhdhhhhhhhhhhdhhhhhhddhdhhhhhhhhdhhhhhhhhdhhhhhdhhhhhhhhddhhhddhhddhhdhhhhhhhhhhhhhhhhdhhhhhhddhdhhhhhhhhdhhhhhdhhhhhhhhhhhhhhhhhhddhhhhhhhhhhhhhhhhh\n",
            "fefllsfllssrleyrleyelefeyrrleyrkekrkllyrlefrkllsllllleyelslellslelelefrlsrllssrleyrllllelefefeyrllllslllelelrsllllleyeyrkekelllrllsfeleslelelefrlsrllssrleyrleyelefeyrllsllllslllllllllrllrlllyllylll\n",
            "a h a e a    ea    a    e      e  a e   e ae a  h     a  a  a a aae    e a e    ea    a    aa a t       a   t aa  a a   t      h  a e aa   e    e a e    ea    a    a t       ae  e a t  a           \n",
            " t  t  t                                                                                                                                                                                             \n",
            " s  s  re          e                                  e         e                     e                                                                        e                                     \n",
            "    e  te  n en    e nr n   hn en  n   ne  n enne  n  aen  enennee e nen      n en nn e nr   ene n n en e  en ennne   nen ne e nn nennen n  e nen      n en    e nr ne nrnnennen  n en e  enn n en  n\n",
            "   ie  ei   ae   e e th  n ei  eehenie  ein t   ee  aine wee e  en e  e  he    ae   eihe   ihe e  e  e  e    i  Bnee ine   ene  ti   we     e  e  he    ae   e e th  e ne  e  e    e   e     e  e    \n",
            "t  e ta  t the there trt  ae t htre t taeae t  t s a e e there    t twl tae t the ta thtre trtae a tr tae  t s th  e sat  he eta  t e e  t t twl tae t the there tataethe t taet  ta taest th the t t\n",
            "     tal  tah tth ae th tl eltthtah t ta th t tah t htnltth wlthl a ew   h   tah tta thtte th whta th ta   wht th le ea t aeh ta  w ehlw t a ew   h   tah tth ae th whth wh wh tl ta tt  w tl th tl t\n",
            " i l whll  ah  to rewto tia   t  se   to  l    ah  th cl th clta  t  wl whll  ah  to to  e t  tl    e tal  wr  to    e ll  ew tr  th hlth  t  wl whll  ah  to rewth tl rl a tb t   dltth th b th  l t\n",
            " icl wrll  wal toerewem  tul  b  hiwkita  l    uh  thicl to c wa  trecl wall  wal tt th re    te      tr   ta  to   rewk wre  tal therlwre trecl wall  wal toereweo te r te tb r rel  wh tre  whick  \n",
            "reck wall  arerthere eu lbu   ai rbud ta   m   ah  ahick th cewae trick wrll  arer o thehe  h to   uh ta   ta  to  erewk  hic tr  thec thertrick wrll  arerthere e  t  r ae to rbail  wh ta i thic'  \n",
            "reck wall  ahe there tu  aue   i  au  th  wu   th  ahick whec the  heck wrll  ahe  o the e eh th  aa  the  the to   recr thec tru thec the  heck wrll  ahe there th ther ae th  truls wh ta l theck  \n",
            "reck walls aae the a to  au   t   au  th  tu   th  ahick thec t e theck wrlls aae to the e th th  aus tr   tre ton  retr thec wal then the theck wrlls aae the a th therrae ta  arlls wa tal  thenk  \n",
            "reck walls tre the e ton au   ton au  tonstu   th  ahick thec the theck walls tre to the e th th  aus ar   tae ton eretnlthec tal then the theck walls tre the e to thirrae ta  trlls wa tall t eck  \n",
            "reck walls tre there ton aue  ton an  tonsmu   to  a eck thin the theck walls tre  o the e to ton ous ar , tae ton  retnlthec tal thin the theck walls tre there tontherras ton aalls wa banl theck  \n",
            "reck walls are there to  aut  ton an  konsmu   to  aheck thec the treck walls are 'o there to ton ous ar , ta ,tonherean then tal when thertreck walls are there tonthew ts ton aalls wa ball theck  \n",
            "reck walls are there to  ante to  an  kon tu t to  ahink then thi trick walls are 'o there th to  aas aa , br ,tonherewn thin wal whan thi trick walls are there to thew ts aon bally we banl think  \n",
            "reck walls are there to  ante th  an  ton wa   tot ahink thin thi brink walls are 'o thire to to   as aa , mr  tonherean thit way what thi brink walls are there to t aw ts aon bally we banl think  \n",
            "reck walls are there to  anre to  anh tou ma t to  bhink thit thirbrink walls are 's there to to   as ta , tu  rot erein thit way thit thirbrink walls are there to t iw ts to  wally we banl think  \n",
            "reck walls are there to tanuers   and tou ma t to  ahitk thit thirbrick walls are 't there to te p as tu , ta  roeherein thit way thit thirbrick walls are there to t aw bs ta  balls we ball thitkt \n",
            "reck walls are there to  anrerso  and you bus  to  aheck thit therbrick walls are 't there to yeep us tu , bu  rheherein thit way thet therbrick walls are there to yhew bs tow badly we tanl theck  \n",
            "reck walls are there ton a reaso  and you mus  to  ahetk thit therbrick walls are 't there to yeep us au , bu  rhtherein thit way that tharbrick walls are there to yhew bs aow badly we tanl theck  \n",
            "reck wayls are there to  a retso  and you mus  no  ahitk that the brick walls are 't the e to yeep us fu , but rht e  in that way that tha brick walls are there to yhew us how badly we banl thenkt \n",
            "reck walls are there tor anreaso  and you must no  ahink that the brick walls are 't the e to yoep us fu , tu  rhnherein thit way that the brick walls are there to yhew us how budly we tanl thenk  \n",
            "reck walls are there tor anreason and you must not ahing that the brick walls are 't the e to yeep us hu , but ranherein thit way that the brick walls are there to yhew ts how badly we ban  thengt \n",
            "reck walls are there tor a reason and you must not think thit the brick walls are 't there to yeep us hu , but ranher in thit way thit the brick walls are there to yhow bs how badly we tanl thenk  \n",
            "reck walls are there tor a reason and you must not think that the brick walls are 't there to yeep us hut, but rather in thit way that the brick walls are there to yhow us how badly we bayl thenk  \n",
            "reck walls are there for a reason and you must not thenk that therbrick walls are 't there to yeep us out, but rather in thit way thas therbrick walls are there fo yhow us how badly we tank thenk  \n",
            "reck walls are there tor a reason and you must not think thas the brick walls are 't there to keep us out, but rather in this way thas the brick walls are there to khow us how bally we wanl thasg  \n",
            "reck walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way thas therbrick walls are there to show us oow badly we tant thenk s\n",
            "reck walls are there tor a reason and you must not think that the brick walls are 't the e to keep us out, but rather in this way that the brick walls are there to khow us oow badly we tant thenkss\n",
            "reck walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us oow badly we bant thenkss\n",
            "reck walls are there tor a reason and you must not think that therbrick walls are 't there to keep us out, but rather in this way that therbrick walls are there to khow us how badly we want thengss\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thengsh\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't the e to seep us out, but rather in this way that the brick walls are there to show us how badly we wan  thengss\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that therbrick walls are there to show us how badly we wadt thenksh\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that therbrick walls are there to khow us how badly we want thenksh\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thingsh\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thengsh\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thengsh\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thenksh\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thenks.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thengs.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there for a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there fo show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there for a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there fo show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there for a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there fo khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "f0IdI9o5Vi6t",
        "outputId": "f72980f2-dd28-453b-c037-1b537978577b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want things.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과가 어떤가요?? 마지막 에폭의 문장이 그럴싸한가요?"
      ],
      "metadata": {
        "id": "PkIzDTdyvTHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task2\n",
        "\n",
        "위 sentence는 제가 임의로 생성한 문장들입니다.\n",
        "\n",
        "마음에 드시는 문구 가져오셔서 문장이 어떻게 생성되는지 확인해보세요! \n",
        "\n",
        "영어가 아닌 한국어로 시도해보는 것도 좋겠죠? \n",
        "\n",
        "수정이 많이 필요(토큰화 등) 할 수 있으나 한번 시도해보시는 것 권장드립니다 :)\n",
        "\n",
        "위 베이스라인은 어디든 수정하셔도 좋고 조금 더 자연스러운 문장이 나올 수 있게 다양한 시도를 해보세요!\n",
        "\n",
        "조건 : 문장 3개 이상, 연결성이 있는 문장을 \" \" 으로 구분하여 ( )에 넣기"
      ],
      "metadata": {
        "id": "kN1zL8Dpvane"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어린왕자-\"어떤 별에 사는 꽃을 좋아한다면\n",
        "밤에 하늘을 쳐다보는 게 즐거울 거야.\n",
        "어느 별이나 다 꽃이 필테니까.\""
      ],
      "metadata": {
        "id": "a94X35YEbmrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "xCxt8CmAsSFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a52c194-09f5-4f8f-83c9-9f447c49ae3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.6/465.6 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"어떤 별에 사는 꽃을 좋아한다. \"\n",
        "            \"밤에 하늘을 쳐다보는 게 즐거울 것이다. \"\n",
        "            \"어느 별이나 다 꽃이 피기 때문이다. \")"
      ],
      "metadata": {
        "id": "3yzHQ7kXbXu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "tokenizer = Okt()\n",
        "word = tokenizer.morphs(sentence)"
      ],
      "metadata": {
        "id": "HUXjcbcZsYiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRX00Nhabp3X",
        "outputId": "be98c0af-21d3-4cba-ed16-dfcef6757e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['어떤',\n",
              " '별',\n",
              " '에',\n",
              " '사는',\n",
              " '꽃',\n",
              " '을',\n",
              " '좋아한다',\n",
              " '.',\n",
              " '밤',\n",
              " '에',\n",
              " '하늘',\n",
              " '을',\n",
              " '쳐다보는',\n",
              " '게',\n",
              " '즐거울',\n",
              " '것',\n",
              " '이다',\n",
              " '.',\n",
              " '어느',\n",
              " '별',\n",
              " '이나',\n",
              " '다',\n",
              " '꽃',\n",
              " '이',\n",
              " '피기',\n",
              " '때문',\n",
              " '이다',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=list(set(word))\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krFfTl25wzbN",
        "outputId": "8cc504aa-3a2e-48f9-a6d2-e83f00b2a675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['게', '별', '때문', '즐거울', '어느', '밤', '좋아한다', '하늘', '어떤', '사는', '이', '.', '이다', '에', '을', '이나', '것', '다', '쳐다보는', '꽃', '피기']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2index = {tkn: i for i, tkn in enumerate(vocab)}\n",
        "print(word2index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE8d9zfqbqN1",
        "outputId": "30ac39a9-9055-4a83-cb50-aed350fa6713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'게': 0, '별': 1, '때문': 2, '즐거울': 3, '어느': 4, '밤': 5, '좋아한다': 6, '하늘': 7, '어떤': 8, '사는': 9, '이': 10, '.': 11, '이다': 12, '에': 13, '을': 14, '이나': 15, '것': 16, '다': 17, '쳐다보는': 18, '꽃': 19, '피기': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(word2index)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUZUeL7hc0TE",
        "outputId": "f1eda1af-0c63-4d25-91bf-8cf2eea59faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index2word = {v: k for k, v in word2index.items()}\n",
        "print(index2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8ch27rnsyIH",
        "outputId": "16cb96ee-44ab-4a40-ddb2-7951e211221d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '게', 1: '별', 2: '때문', 3: '즐거울', 4: '어느', 5: '밤', 6: '좋아한다', 7: '하늘', 8: '어떤', 9: '사는', 10: '이', 11: '.', 12: '이다', 13: '에', 14: '을', 15: '이나', 16: '것', 17: '다', 18: '쳐다보는', 19: '꽃', 20: '피기'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_data(word, word2index):\n",
        "    encoded = [word2index[token] for token in word]  \n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] \n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0)\n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) \n",
        "    return input_seq, label_seq"
      ],
      "metadata": {
        "id": "B1xYNpD7syPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = build_data(word, word2index)"
      ],
      "metadata": {
        "id": "LBWWpkiatjKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS442OoqxKDa",
        "outputId": "6338377a-8f38-42cd-a274-ccb7ca21bf37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 8,  1, 13,  9, 19, 14,  6, 11,  5, 13,  7, 14, 18,  0,  3, 16, 12, 11,\n",
            "          4,  1, 15, 17, 19, 10, 20,  2, 12]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBwSROsaxL_B",
        "outputId": "c35e1b43-cb70-4600-bfd7-c8e8f6010541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 13,  9, 19, 14,  6, 11,  5, 13,  7, 14, 18,  0,  3, 16, 12, 11,  4,\n",
            "          1, 15, 17, 19, 10, 20,  2, 12, 11]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, \n",
        "                                            embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, \n",
        "                                batch_first=batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) \n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding_layer(x)\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        output = self.linear(output)\n",
        "        return output.view(-1, output.size(2))"
      ],
      "metadata": {
        "id": "sWis9BrexMDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2index)  \n",
        "input_size = 5  \n",
        "hidden_size = 20  "
      ],
      "metadata": {
        "id": "RNYBG0eGxWrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "loss_function = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "metadata": {
        "id": "4kIcI-YTxWvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(X)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ7oggC1xWzl",
        "outputId": "c7764f45-12e3-4a0a-c680-b9fd0b662867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.7028e-01, -3.0536e-01,  2.3361e-02,  1.4059e-01, -4.3671e-01,\n",
            "         -9.8117e-02, -2.2294e-01, -2.6260e-01,  3.2985e-01,  2.8858e-01,\n",
            "          3.4434e-01,  5.7208e-02,  1.2566e-01,  1.2429e-01,  1.3854e-01,\n",
            "         -1.2091e-01, -1.2504e-01, -3.4621e-01, -8.7800e-02, -1.3401e-01,\n",
            "         -1.6946e-02],\n",
            "        [-2.5868e-01, -2.3050e-01,  7.1767e-02,  4.3174e-01, -9.4876e-02,\n",
            "         -2.8475e-01, -1.7081e-02, -2.7850e-01,  3.3905e-01,  2.3602e-01,\n",
            "          4.8124e-01,  1.1830e-01,  2.7212e-01, -6.7683e-02,  5.6648e-01,\n",
            "         -1.5716e-01,  1.9822e-01, -2.7959e-01, -5.8905e-02,  4.0438e-02,\n",
            "         -1.3452e-02],\n",
            "        [-1.2611e-01,  3.2429e-01,  9.3503e-02, -1.4390e-01, -5.0301e-01,\n",
            "         -2.1652e-03, -1.2540e-01, -5.8548e-01,  6.8160e-01,  2.2887e-01,\n",
            "          4.1531e-02, -2.7737e-01, -2.1237e-01, -1.9363e-01,  9.6809e-02,\n",
            "          8.6490e-02, -2.4485e-01, -4.5793e-01, -8.2795e-02, -5.8530e-02,\n",
            "          1.9544e-02],\n",
            "        [-9.6225e-03, -3.5054e-02,  1.9974e-01,  3.2282e-01,  7.1222e-04,\n",
            "         -2.2037e-01,  2.4614e-01, -2.0916e-01,  6.2713e-02,  2.0031e-01,\n",
            "          2.7966e-01,  3.1685e-01,  3.3071e-01, -1.0397e-01,  6.7796e-01,\n",
            "         -2.1862e-01,  9.8015e-02, -3.2770e-01,  1.5660e-01,  9.2914e-02,\n",
            "         -1.2881e-01],\n",
            "        [ 2.9334e-02,  2.0791e-01,  2.1901e-01, -4.0901e-02, -1.5783e-02,\n",
            "         -1.2618e-01, -4.4852e-02, -1.6554e-01,  8.3377e-02,  1.4529e-01,\n",
            "          8.5250e-03,  8.4258e-02,  2.0862e-02, -2.2704e-02,  2.7329e-01,\n",
            "          1.1817e-02, -3.7438e-01, -3.8017e-01, -2.4662e-02, -1.1440e-01,\n",
            "         -8.2268e-02],\n",
            "        [ 2.8161e-02, -6.7128e-02,  4.9652e-02,  2.5137e-01, -3.6002e-01,\n",
            "         -1.2382e-01,  2.8651e-01, -4.3138e-01,  2.8889e-01,  3.6853e-01,\n",
            "          3.3848e-01,  2.0737e-01,  3.3884e-01,  3.5989e-02,  4.7831e-02,\n",
            "         -3.9942e-01,  5.7776e-02, -8.8900e-02,  3.8344e-01, -6.1553e-02,\n",
            "         -7.2696e-02],\n",
            "        [-1.2639e-01,  7.9569e-02,  4.2827e-01,  1.1935e-01, -1.1429e-01,\n",
            "          1.1228e-02,  6.4828e-02, -2.1552e-01,  1.7313e-01,  2.4849e-01,\n",
            "          1.8498e-02,  3.0703e-01, -1.1076e-02, -1.7640e-01,  3.2468e-01,\n",
            "          2.0189e-02, -8.2220e-02, -2.7484e-01,  1.0254e-01, -1.9805e-02,\n",
            "         -2.0802e-02],\n",
            "        [ 3.0074e-02,  4.1367e-01,  1.6170e-01, -4.4058e-02, -3.1451e-01,\n",
            "          1.2076e-01,  3.2918e-01, -5.8349e-01,  1.6167e-01,  3.2888e-01,\n",
            "         -2.8436e-02,  2.6075e-01,  3.1364e-02, -1.7551e-01, -8.7076e-02,\n",
            "         -2.2168e-01, -3.5280e-01, -6.0282e-02,  5.7305e-01,  1.6283e-03,\n",
            "         -4.1046e-02],\n",
            "        [-6.9541e-02,  1.2017e-01,  5.2621e-01,  2.9476e-01,  1.6274e-01,\n",
            "         -6.3834e-02,  3.8736e-01, -7.2919e-02,  2.6131e-02,  4.3636e-01,\n",
            "          2.2465e-01,  5.2852e-01,  4.0472e-01, -1.7411e-01,  6.4123e-01,\n",
            "         -1.1802e-01,  1.0363e-01, -2.0450e-01,  5.7339e-01,  1.4972e-01,\n",
            "         -1.2585e-01],\n",
            "        [-1.2273e-01,  1.5502e-01,  1.3436e-01, -5.1430e-02, -3.9424e-01,\n",
            "         -5.6630e-03, -1.1658e-01, -3.7587e-01,  6.6011e-01,  4.3530e-01,\n",
            "          2.0211e-01, -1.6270e-01,  1.0659e-02, -1.5704e-01, -2.2414e-02,\n",
            "          2.8752e-02, -3.1756e-01, -2.9047e-01,  1.2713e-01, -1.2329e-01,\n",
            "         -5.8137e-02],\n",
            "        [-1.2691e-01,  2.4262e-01,  2.2362e-01,  2.8141e-01, -1.7214e-01,\n",
            "          2.3037e-01,  8.5834e-02, -5.1344e-01,  2.2474e-01,  2.7846e-01,\n",
            "         -1.4369e-01,  2.9346e-01, -1.6773e-01, -4.8818e-01,  3.5293e-01,\n",
            "          6.2378e-02,  4.6746e-02, -8.1897e-02,  9.2131e-02,  2.0761e-01,\n",
            "          2.2997e-02],\n",
            "        [-5.0116e-02, -2.7005e-02,  1.7014e-01,  6.4467e-02, -5.0341e-01,\n",
            "         -7.6308e-03,  3.6318e-01, -4.2917e-01,  3.9537e-01,  4.8808e-01,\n",
            "          3.2918e-01,  1.6635e-01,  1.8393e-01,  8.1049e-02, -4.0244e-02,\n",
            "         -3.7171e-01, -1.4262e-01, -1.0860e-01,  4.0781e-01, -2.6454e-01,\n",
            "          1.5036e-02],\n",
            "        [-9.0826e-02, -1.2726e-01,  3.8377e-01,  9.8007e-02, -3.1108e-01,\n",
            "         -3.0248e-02,  2.0613e-01, -3.4830e-01,  8.7731e-02,  2.2736e-01,\n",
            "          2.3135e-01,  3.7279e-01,  3.9587e-02,  9.2946e-02,  1.4814e-01,\n",
            "         -2.5114e-01, -2.0602e-01, -2.2489e-01,  2.8075e-01, -1.0482e-01,\n",
            "          1.0312e-01],\n",
            "        [-1.5286e-01,  1.8990e-01,  3.1015e-01,  3.3125e-01,  1.3103e-01,\n",
            "         -1.7623e-03,  3.2750e-02, -1.5775e-01,  1.0551e-01,  5.3238e-01,\n",
            "          4.6961e-02,  2.6979e-01,  1.5057e-01, -2.5970e-01,  5.6218e-01,\n",
            "          9.2396e-02,  3.2873e-02, -2.6229e-01,  2.7425e-01,  1.0057e-01,\n",
            "         -9.5535e-02],\n",
            "        [-1.8117e-01, -3.2086e-02,  1.2199e-01,  9.8141e-04, -3.6133e-01,\n",
            "         -6.4439e-02, -3.2873e-02, -3.8193e-01,  3.7317e-01,  3.0066e-01,\n",
            "          2.9875e-01,  4.0610e-02,  1.2863e-01,  8.7244e-02,  1.1816e-01,\n",
            "         -1.3368e-01, -2.7540e-01, -3.6103e-01,  4.0450e-02, -1.5611e-01,\n",
            "          6.7314e-02],\n",
            "        [-1.4400e-01, -1.2443e-01,  2.3197e-01,  4.3023e-01,  1.6681e-02,\n",
            "         -2.5288e-01,  1.3655e-01, -1.8461e-01,  1.6849e-01,  3.3877e-01,\n",
            "          4.0405e-01,  2.4597e-01,  3.3643e-01, -7.1097e-02,  6.5438e-01,\n",
            "         -1.9168e-01,  1.8789e-01, -2.7782e-01,  1.5654e-01,  7.1498e-02,\n",
            "         -5.1646e-02],\n",
            "        [-1.0916e-01, -5.5773e-02,  1.5962e-01,  1.9191e-01,  5.2845e-02,\n",
            "         -3.4125e-01, -1.1823e-01, -8.3533e-02,  1.5966e-01,  1.6375e-01,\n",
            "          3.4904e-01, -5.1252e-03,  1.9383e-01,  7.2115e-02,  5.3655e-01,\n",
            "         -1.2100e-01, -1.2985e-01, -3.9090e-01, -2.4520e-01, -7.9401e-02,\n",
            "          1.2694e-02],\n",
            "        [ 1.2896e-01,  5.8267e-01,  4.0076e-03, -4.5501e-02, -3.8081e-01,\n",
            "          1.2343e-01,  2.2688e-01, -7.1861e-01,  1.6085e-01,  1.1791e-01,\n",
            "         -1.4812e-01,  6.1018e-02, -1.2604e-01, -2.7844e-01, -6.6479e-02,\n",
            "         -1.3847e-01, -3.6698e-01, -1.1719e-01,  3.2104e-01,  8.6876e-02,\n",
            "         -5.2714e-02],\n",
            "        [-7.5721e-02, -6.1212e-02,  3.6976e-01,  8.2636e-02, -4.1575e-01,\n",
            "          1.3405e-01,  2.5445e-01, -1.9706e-01,  2.3067e-01,  4.7273e-01,\n",
            "          8.1688e-02,  4.1053e-01,  1.0382e-01, -3.7263e-02,  7.9704e-02,\n",
            "         -7.8017e-02, -4.1388e-02, -2.4216e-01,  4.6563e-01, -1.2361e-01,\n",
            "         -1.4729e-01],\n",
            "        [-3.1191e-01, -3.9422e-01,  2.3695e-01,  3.2633e-01, -1.2409e-01,\n",
            "         -2.6725e-01,  1.2393e-02, -1.7686e-01,  4.4242e-01,  4.3831e-01,\n",
            "          5.2072e-01,  2.2151e-01,  4.3681e-01,  6.7433e-02,  4.1299e-01,\n",
            "         -1.7651e-01,  1.2628e-01, -2.7314e-01,  1.6138e-01, -1.1918e-01,\n",
            "         -4.5781e-02],\n",
            "        [-1.3990e-01,  2.0042e-01,  2.4854e-01,  3.5269e-02, -1.8447e-01,\n",
            "         -7.1078e-02,  4.5782e-03, -4.9978e-01,  1.5779e-01,  1.3079e-01,\n",
            "          1.2608e-01,  1.0158e-01, -8.5665e-02, -7.0989e-03,  2.6438e-01,\n",
            "         -1.3399e-01, -2.6200e-01, -3.1869e-01,  1.0047e-03, -2.0831e-02,\n",
            "          2.1253e-01],\n",
            "        [-2.3484e-02, -3.5044e-01,  1.1750e-01,  3.6018e-01, -1.4663e-01,\n",
            "         -2.8590e-01,  1.4035e-01, -4.8913e-02, -8.9788e-02,  2.7540e-01,\n",
            "          5.1209e-01,  2.8418e-01,  3.3178e-01,  2.8973e-01,  4.3465e-01,\n",
            "         -3.6871e-01, -1.8618e-04, -3.1599e-01,  9.1753e-02, -8.8584e-02,\n",
            "         -2.2633e-02],\n",
            "        [-6.5712e-02,  8.5194e-02,  2.1327e-01,  2.4207e-01,  1.4383e-01,\n",
            "         -2.0912e-01, -5.9656e-02, -1.2072e-01, -4.7002e-02,  9.2673e-02,\n",
            "          1.3320e-01,  2.0745e-01,  1.3296e-01, -7.2841e-02,  5.2026e-01,\n",
            "          1.7587e-02, -1.5219e-01, -3.5653e-01, -7.9362e-02,  4.9468e-02,\n",
            "         -6.2674e-02],\n",
            "        [ 2.4634e-01,  2.5786e-01,  1.6688e-01,  5.9653e-01,  4.1620e-01,\n",
            "         -1.4053e-01,  4.5713e-01, -3.6757e-02, -1.3395e-01,  2.7886e-01,\n",
            "          1.4339e-01,  4.5155e-01,  3.6341e-01, -3.8862e-01,  5.1975e-01,\n",
            "         -2.3730e-01,  3.3504e-01,  1.4557e-01,  2.4193e-01,  2.8473e-01,\n",
            "         -2.0528e-01],\n",
            "        [ 1.0603e-01,  2.1366e-01,  3.5311e-01,  4.3068e-02, -2.1246e-01,\n",
            "          1.1701e-01,  2.6505e-01, -2.4472e-01,  2.0153e-01,  2.6338e-01,\n",
            "         -4.6652e-02,  3.2259e-01, -4.5462e-03, -1.6622e-01, -5.4695e-02,\n",
            "         -1.3615e-01, -1.6244e-01, -3.5444e-02,  1.3543e-01, -1.2937e-01,\n",
            "          8.5225e-03],\n",
            "        [-5.8026e-02,  3.3877e-01,  4.2218e-01,  3.6679e-01,  3.2258e-01,\n",
            "          2.4814e-01, -5.2363e-02, -1.3780e-01, -4.8858e-03,  4.6583e-01,\n",
            "         -3.9836e-01,  4.7250e-01, -8.3828e-02, -6.3372e-01,  7.1970e-01,\n",
            "          3.3808e-01,  8.5669e-02, -2.3032e-01,  1.0418e-01,  2.8023e-01,\n",
            "         -1.3629e-01],\n",
            "        [-1.2901e-01, -1.4081e-01,  5.9912e-02,  2.6779e-01,  8.0780e-02,\n",
            "         -2.0007e-01, -5.6449e-02, -5.4726e-02,  2.1342e-01,  3.9724e-01,\n",
            "          3.8030e-01,  1.3591e-01,  3.8750e-01, -1.8080e-02,  5.0999e-01,\n",
            "         -1.5274e-01, -9.1612e-02, -2.8487e-01, -1.1911e-01, -1.4992e-01,\n",
            "         -4.9515e-02]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode = lambda y: [index2word.get(x) for x in y]"
      ],
      "metadata": {
        "id": "ht1-fBtVxW3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(201):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X)\n",
        "    loss = loss_function(output, Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 40 == 0:\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMraalWyxW6W",
        "outputId": "ee3cc209-a9ca-4393-8080-19b6206afc81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/201] 3.0875 \n",
            "Repeat 이 을 어떤 을 을 쳐다보는 때문 쳐다보는 을 어떤 을 사는 때문 을 어떤 을 을 별 사는 이 을 이 을 즐거울 때문 을 을\n",
            "\n",
            "[41/201] 2.6681 \n",
            "Repeat 사는 을 별 을 . 좋아한다 . 별 이다 사는 을 쳐다보는 . 을 . 이다 . 별 때문 . . 이 . 꽃 . . .\n",
            "\n",
            "[81/201] 2.1478 \n",
            "Repeat 사는 꽃 사는 꽃 . 좋아한다 . 밤 이다 하늘 을 쳐다보는 게 즐거울 . 꽃 . 어느 별 . 다 꽃 . 꽃 때문 이다 .\n",
            "\n",
            "[121/201] 1.5778 \n",
            "Repeat 사는 꽃 사는 꽃 . 좋아한다 . 밤 이다 하늘 을 쳐다보는 게 즐거울 것 꽃 . 어느 별 . 다 꽃 이 꽃 때문 이다 .\n",
            "\n",
            "[161/201] 1.0894 \n",
            "Repeat 별 꽃 사는 꽃 을 좋아한다 . 밤 에 하늘 을 쳐다보는 게 즐거울 것 이다 . 어느 별 . 다 꽃 이 피기 때문 이다 .\n",
            "\n",
            "[201/201] 0.7205 \n",
            "Repeat 별 에 사는 꽃 을 좋아한다 . 밤 에 하늘 을 쳐다보는 게 즐거울 것 이다 . 어느 별 이나 다 꽃 이 피기 때문 이다 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "task1 그대로"
      ],
      "metadata": {
        "id": "EVJt8NIig6BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = vocab_size \n",
        "sequence_length = 10\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "E_WcyIo7djRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(vocab) - sequence_length):\n",
        "  x_str = vocab[i:i+sequence_length]\n",
        "  y_str = vocab[i+1:i+sequence_length+1]\n",
        "  print(i, x_str, \"->\", y_str)\n",
        "\n",
        "  x_data.append([word2index[c] for c in x_str])\n",
        "  y_data.append([word2index[d] for d in y_str])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PGsFZ0Hd0df",
        "outputId": "09b056db-31af-438c-e6ec-7f2c3e13dc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ['게', '별', '때문', '즐거울', '어느', '밤', '좋아한다', '하늘', '어떤', '사는'] -> ['별', '때문', '즐거울', '어느', '밤', '좋아한다', '하늘', '어떤', '사는', '이']\n",
            "1 ['별', '때문', '즐거울', '어느', '밤', '좋아한다', '하늘', '어떤', '사는', '이'] -> ['때문', '즐거울', '어느', '밤', '좋아한다', '하늘', '어떤', '사는', '이', '.']\n",
            "2 ['때문', '즐거울', '어느', '밤', '좋아한다', '하늘', '어떤', '사는', '이', '.'] -> ['즐거울', '어느', '밤', '좋아한다', '하늘', '어떤', '사는', '이', '.', '이다']\n",
            "3 ['즐거울', '어느', '밤', '좋아한다', '하늘', '어떤', '사는', '이', '.', '이다'] -> ['어느', '밤', '좋아한다', '하늘', '어떤', '사는', '이', '.', '이다', '에']\n",
            "4 ['어느', '밤', '좋아한다', '하늘', '어떤', '사는', '이', '.', '이다', '에'] -> ['밤', '좋아한다', '하늘', '어떤', '사는', '이', '.', '이다', '에', '을']\n",
            "5 ['밤', '좋아한다', '하늘', '어떤', '사는', '이', '.', '이다', '에', '을'] -> ['좋아한다', '하늘', '어떤', '사는', '이', '.', '이다', '에', '을', '이나']\n",
            "6 ['좋아한다', '하늘', '어떤', '사는', '이', '.', '이다', '에', '을', '이나'] -> ['하늘', '어떤', '사는', '이', '.', '이다', '에', '을', '이나', '것']\n",
            "7 ['하늘', '어떤', '사는', '이', '.', '이다', '에', '을', '이나', '것'] -> ['어떤', '사는', '이', '.', '이다', '에', '을', '이나', '것', '다']\n",
            "8 ['어떤', '사는', '이', '.', '이다', '에', '을', '이나', '것', '다'] -> ['사는', '이', '.', '이다', '에', '을', '이나', '것', '다', '쳐다보는']\n",
            "9 ['사는', '이', '.', '이다', '에', '을', '이나', '것', '다', '쳐다보는'] -> ['이', '.', '이다', '에', '을', '이나', '것', '다', '쳐다보는', '꽃']\n",
            "10 ['이', '.', '이다', '에', '을', '이나', '것', '다', '쳐다보는', '꽃'] -> ['.', '이다', '에', '을', '이나', '것', '다', '쳐다보는', '꽃', '피기']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OOk5_05d5Vi",
        "outputId": "5f5b8d85-ef42-4e39-aa1c-0c682fb865c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZqneXlxeb6-",
        "outputId": "0833257f-a4f7-44f4-f37f-a8cccf6b634c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
              " [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
              " [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
              " [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
              " [4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
              " [5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
              " [6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
              " [7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n",
              " [8, 9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
              " [9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
              " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "PB7nOvEHeDWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9vXOsNge1On",
        "outputId": "cdca0166-a66c-4db4-cfd9-ba1a0d76972f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([11, 10, 21])\n",
            "레이블의 크기 : torch.Size([11, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouvsYH_keSW7",
        "outputId": "179a3c26-1419-4340-d4ba-4b10f1d55bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyLfQ-ixeoSx",
        "outputId": "506e4e1c-18b6-4750-948d-780689325cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers = layers, batch_first = True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "l1wqNyA8ep2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 2)"
      ],
      "metadata": {
        "id": "T9gprjaRexkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "uHaUHvDze652"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQEGA3mke-mH",
        "outputId": "7f17324d-787f-45b9-c46f-c94c2d70cdc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 10, 21])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: \n",
        "            predict_str += ''.join([vocab[t] for t in result])\n",
        "        else: \n",
        "            predict_str += vocab[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwUSrZo3fBLQ",
        "outputId": "9f6627ca-c653-41e3-f0c7-8ecca1ff6aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "것것것것것다다것이것것이나것이것것이것것것\n",
            "이사는이다사는사는다다이사는이사는이사는이이다사는이사는이이\n",
            "이이다이다이다이다이다이다이다사는이사는이다사는이이다사는이다사는이다사는\n",
            "이이다이다이다이다이다이다이다사는이사는이다사는이다이다이다이다사는이다사는\n",
            "이이다이다이다이다이다이다이다사는이사는이다사는이다이다이다이다사는이다사는\n",
            "이이다이다이다이다이다이다이다사는이사는이다에이이다이다이다이이다사는\n",
            "이이다이다이다에이다.어떤사는이.이다에이이다에에이이다사는\n",
            "이이다이다이다에.하늘어떤사는이.이다에을이다것을이이다사는\n",
            "이이다이다에을좋아한다하늘어떤사는이.이다에을것것을을어떤사는\n",
            "이이다이다에을좋아한다하늘어떤사는이.이다에을것것을별어떤사는\n",
            "이이다이다에을좋아한다하늘어떤사는이.이다에을것것을별어떤좋아한다\n",
            "이이다이다어떤을좋아한다하늘어떤사는이.이다에을것것별별별좋아한다\n",
            "이사는이다에을좋아한다하늘어떤사는이.이다에을것것별별별피기\n",
            "이사는즐거울에밤좋아한다하늘어떤사는이.이다에을것것별별별피기\n",
            "이을즐거울어떤밤좋아한다하늘어떤사는이.이다에을것것별별별피기\n",
            "이좋아한다즐거울어떤밤좋아한다하늘어떤사는이.이다에을것것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어떤밤좋아한다하늘어떤사는이.이다에을것것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어떤밤좋아한다하늘어떤사는이.이다에을것것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어떤밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어떤밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "을좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "을좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "을좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "다좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "다좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "다좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "다좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "다좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "이좋아한다즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "이때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "이때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "어느때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "어느때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "어느때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "어느때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "어느때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "어느때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "어느때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "어느때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n",
            "별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oJzhFVemfMIC",
        "outputId": "27e8b9c6-7ba0-441c-eb87-2626ab8ff3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'별때문즐거울어느밤좋아한다하늘어떤사는이.이다에을이나것다쳐다보는꽃피기'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}