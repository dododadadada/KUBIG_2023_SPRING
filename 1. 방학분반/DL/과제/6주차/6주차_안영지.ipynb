{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nmQ5F7UAeKB_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task1\n",
        "\n",
        "빈 칸을 채워주세요!\n",
        "\n",
        "단계별 output이 github 파일에는 남아있으니 그 output과 동일한 형태인지 확인하면서 진행해주시면 됩니다~"
      ],
      "metadata": {
        "id": "Sgxd6SxmeVcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 생성할 문장 데이터\n",
        "\n",
        "sentence = (\"Brick walls are there for a reason and you must not think \"\n",
        "            \"that the brick walls aren't there to keep us out, but rather \"\n",
        "            \"in this way that the brick walls are there to show us how badly we want things.\")"
      ],
      "metadata": {
        "id": "NDvUeC8BoUb6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. 문자 집합 만들기\n",
        "world_set = list(set(sentence))\n",
        "\n",
        "## 문제(1): 각 문자에 정수 인코딩 (공백도 하나의 원소로 포함)\n",
        "vocab = {char: idx for idx, char in enumerate(world_set)}"
      ],
      "metadata": {
        "id": "b9lkrKyZf8ie"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_0we5Y-gYDq",
        "outputId": "ce69ddf8-cd51-4d92-82ff-5007d8245e8e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d': 0, 'w': 1, 'n': 2, ' ': 3, 'f': 4, 's': 5, 'm': 6, ',': 7, 'y': 8, 'k': 9, 'h': 10, 'o': 11, 't': 12, 'u': 13, 'c': 14, 'g': 15, 'l': 16, \"'\": 17, 'i': 18, 'b': 19, 'B': 20, 'r': 21, 'a': 22, 'p': 23, 'e': 24, '.': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. 문자 집합 크기 확인\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('문자 집합 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpKupU6lgpfT",
        "outputId": "c7f8ee81-a322-4956-c60c-e961cd1ca4dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 크기 : 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
        "\n",
        "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
        "sequence_length = 10  # 너무 길거나 너무 짧게 잡으면 안됩니다!\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "wFDZJHSMg9In"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. seqence 길이 단위 자르기\n",
        "\n",
        "# 데이터 구성을 위한 리스트\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "## 문제(2): 반복문 내에서의 인덱싱을 사용하여 sequence_length 값 단위로 샘플을 잘라 데이터 만들기, y_str은 x_str은 한 칸씩 쉬프트된 sequnce\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "  x_str = sentence[i:i+sequence_length]\n",
        "  y_str = sentence[i+1:i+sequence_length+1]\n",
        "  print(i, x_str, \"->\", y_str)\n",
        "\n",
        "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
        "  x_data.append([vocab[c] for c in x_str])\n",
        "  y_data.append([vocab[d] for d in y_str])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbDcmJmghN7V",
        "outputId": "72f34eb3-c7d6-4258-eb10-14a37b2b71ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Brick wall -> rick walls\n",
            "1 rick walls -> ick walls \n",
            "2 ick walls  -> ck walls a\n",
            "3 ck walls a -> k walls ar\n",
            "4 k walls ar ->  walls are\n",
            "5  walls are -> walls are \n",
            "6 walls are  -> alls are t\n",
            "7 alls are t -> lls are th\n",
            "8 lls are th -> ls are the\n",
            "9 ls are the -> s are ther\n",
            "10 s are ther ->  are there\n",
            "11  are there -> are there \n",
            "12 are there  -> re there f\n",
            "13 re there f -> e there fo\n",
            "14 e there fo ->  there for\n",
            "15  there for -> there for \n",
            "16 there for  -> here for a\n",
            "17 here for a -> ere for a \n",
            "18 ere for a  -> re for a r\n",
            "19 re for a r -> e for a re\n",
            "20 e for a re ->  for a rea\n",
            "21  for a rea -> for a reas\n",
            "22 for a reas -> or a reaso\n",
            "23 or a reaso -> r a reason\n",
            "24 r a reason ->  a reason \n",
            "25  a reason  -> a reason a\n",
            "26 a reason a ->  reason an\n",
            "27  reason an -> reason and\n",
            "28 reason and -> eason and \n",
            "29 eason and  -> ason and y\n",
            "30 ason and y -> son and yo\n",
            "31 son and yo -> on and you\n",
            "32 on and you -> n and you \n",
            "33 n and you  ->  and you m\n",
            "34  and you m -> and you mu\n",
            "35 and you mu -> nd you mus\n",
            "36 nd you mus -> d you must\n",
            "37 d you must ->  you must \n",
            "38  you must  -> you must n\n",
            "39 you must n -> ou must no\n",
            "40 ou must no -> u must not\n",
            "41 u must not ->  must not \n",
            "42  must not  -> must not t\n",
            "43 must not t -> ust not th\n",
            "44 ust not th -> st not thi\n",
            "45 st not thi -> t not thin\n",
            "46 t not thin ->  not think\n",
            "47  not think -> not think \n",
            "48 not think  -> ot think t\n",
            "49 ot think t -> t think th\n",
            "50 t think th ->  think tha\n",
            "51  think tha -> think that\n",
            "52 think that -> hink that \n",
            "53 hink that  -> ink that t\n",
            "54 ink that t -> nk that th\n",
            "55 nk that th -> k that the\n",
            "56 k that the ->  that the \n",
            "57  that the  -> that the b\n",
            "58 that the b -> hat the br\n",
            "59 hat the br -> at the bri\n",
            "60 at the bri -> t the bric\n",
            "61 t the bric ->  the brick\n",
            "62  the brick -> the brick \n",
            "63 the brick  -> he brick w\n",
            "64 he brick w -> e brick wa\n",
            "65 e brick wa ->  brick wal\n",
            "66  brick wal -> brick wall\n",
            "67 brick wall -> rick walls\n",
            "68 rick walls -> ick walls \n",
            "69 ick walls  -> ck walls a\n",
            "70 ck walls a -> k walls ar\n",
            "71 k walls ar ->  walls are\n",
            "72  walls are -> walls aren\n",
            "73 walls aren -> alls aren'\n",
            "74 alls aren' -> lls aren't\n",
            "75 lls aren't -> ls aren't \n",
            "76 ls aren't  -> s aren't t\n",
            "77 s aren't t ->  aren't th\n",
            "78  aren't th -> aren't the\n",
            "79 aren't the -> ren't ther\n",
            "80 ren't ther -> en't there\n",
            "81 en't there -> n't there \n",
            "82 n't there  -> 't there t\n",
            "83 't there t -> t there to\n",
            "84 t there to ->  there to \n",
            "85  there to  -> there to k\n",
            "86 there to k -> here to ke\n",
            "87 here to ke -> ere to kee\n",
            "88 ere to kee -> re to keep\n",
            "89 re to keep -> e to keep \n",
            "90 e to keep  ->  to keep u\n",
            "91  to keep u -> to keep us\n",
            "92 to keep us -> o keep us \n",
            "93 o keep us  ->  keep us o\n",
            "94  keep us o -> keep us ou\n",
            "95 keep us ou -> eep us out\n",
            "96 eep us out -> ep us out,\n",
            "97 ep us out, -> p us out, \n",
            "98 p us out,  ->  us out, b\n",
            "99  us out, b -> us out, bu\n",
            "100 us out, bu -> s out, but\n",
            "101 s out, but ->  out, but \n",
            "102  out, but  -> out, but r\n",
            "103 out, but r -> ut, but ra\n",
            "104 ut, but ra -> t, but rat\n",
            "105 t, but rat -> , but rath\n",
            "106 , but rath ->  but rathe\n",
            "107  but rathe -> but rather\n",
            "108 but rather -> ut rather \n",
            "109 ut rather  -> t rather i\n",
            "110 t rather i ->  rather in\n",
            "111  rather in -> rather in \n",
            "112 rather in  -> ather in t\n",
            "113 ather in t -> ther in th\n",
            "114 ther in th -> her in thi\n",
            "115 her in thi -> er in this\n",
            "116 er in this -> r in this \n",
            "117 r in this  ->  in this w\n",
            "118  in this w -> in this wa\n",
            "119 in this wa -> n this way\n",
            "120 n this way ->  this way \n",
            "121  this way  -> this way t\n",
            "122 this way t -> his way th\n",
            "123 his way th -> is way tha\n",
            "124 is way tha -> s way that\n",
            "125 s way that ->  way that \n",
            "126  way that  -> way that t\n",
            "127 way that t -> ay that th\n",
            "128 ay that th -> y that the\n",
            "129 y that the ->  that the \n",
            "130  that the  -> that the b\n",
            "131 that the b -> hat the br\n",
            "132 hat the br -> at the bri\n",
            "133 at the bri -> t the bric\n",
            "134 t the bric ->  the brick\n",
            "135  the brick -> the brick \n",
            "136 the brick  -> he brick w\n",
            "137 he brick w -> e brick wa\n",
            "138 e brick wa ->  brick wal\n",
            "139  brick wal -> brick wall\n",
            "140 brick wall -> rick walls\n",
            "141 rick walls -> ick walls \n",
            "142 ick walls  -> ck walls a\n",
            "143 ck walls a -> k walls ar\n",
            "144 k walls ar ->  walls are\n",
            "145  walls are -> walls are \n",
            "146 walls are  -> alls are t\n",
            "147 alls are t -> lls are th\n",
            "148 lls are th -> ls are the\n",
            "149 ls are the -> s are ther\n",
            "150 s are ther ->  are there\n",
            "151  are there -> are there \n",
            "152 are there  -> re there t\n",
            "153 re there t -> e there to\n",
            "154 e there to ->  there to \n",
            "155  there to  -> there to s\n",
            "156 there to s -> here to sh\n",
            "157 here to sh -> ere to sho\n",
            "158 ere to sho -> re to show\n",
            "159 re to show -> e to show \n",
            "160 e to show  ->  to show u\n",
            "161  to show u -> to show us\n",
            "162 to show us -> o show us \n",
            "163 o show us  ->  show us h\n",
            "164  show us h -> show us ho\n",
            "165 show us ho -> how us how\n",
            "166 how us how -> ow us how \n",
            "167 ow us how  -> w us how b\n",
            "168 w us how b ->  us how ba\n",
            "169  us how ba -> us how bad\n",
            "170 us how bad -> s how badl\n",
            "171 s how badl ->  how badly\n",
            "172  how badly -> how badly \n",
            "173 how badly  -> ow badly w\n",
            "174 ow badly w -> w badly we\n",
            "175 w badly we ->  badly we \n",
            "176  badly we  -> badly we w\n",
            "177 badly we w -> adly we wa\n",
            "178 adly we wa -> dly we wan\n",
            "179 dly we wan -> ly we want\n",
            "180 ly we want -> y we want \n",
            "181 y we want  ->  we want t\n",
            "182  we want t -> we want th\n",
            "183 we want th -> e want thi\n",
            "184 e want thi ->  want thin\n",
            "185  want thin -> want thing\n",
            "186 want thing -> ant things\n",
            "187 ant things -> nt things.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVFlILiOixdc",
        "outputId": "6d1b441e-3083-4047-fc08-b696347b07d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 21, 18, 14, 9, 3, 1, 22, 16, 16]\n",
            "[21, 18, 14, 9, 3, 1, 22, 16, 16, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
        "\n",
        "## 문제(4) : x_data를 원핫인코딩 > numpy의 eye를 쓸 수 있지 않을까?\n",
        "x_one_hot = [np.eye(len(vocab))[x] for x in x_data]\n",
        "\n",
        "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
        "\n",
        "## 문제(5) : x_one_hot과 y_data 텐서로 변환 : 둘 다 같은 형식의 텐서로 변환하면 될까?? (FloatTensor, LongTesor 중 맞는 것은?)\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "5lPes1dvjlNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a786abca-2fdb-4d56-ad67-395f878f2760"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-c05613ebc72f>:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##8. 크기 확인\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMZzZlaymMk8",
        "outputId": "025631d5-a667-41db-94a7-d0b611a2891f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([188, 10, 26])\n",
            "레이블의 크기 : torch.Size([188, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##9.원핫인코딩 결과 샘플 확인하기\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knx1DE_AmSFB",
        "outputId": "3f7e144d-eab9-40f1-8cc2-fbc3fbf50582"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##10. 레이블 데이터 샘플 확인하기\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pWDiH1SmYT_",
        "outputId": "aefaa1b0-52ca-4951-8425-ac3b080959f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([21, 18, 14,  9,  3,  1, 22, 16, 16,  5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##11. RNN 모델 구현\n",
        "\n",
        "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
        "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
        "### batch_fisrt 설정 필요할까? (유튜브 강의 참고)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, input_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    rnn_out, hidden = self.rnn(x)\n",
        "    x= self.fc(rnn_out)\n",
        "    return x"
      ],
      "metadata": {
        "id": "-Ww22xu8mfUc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 2)"
      ],
      "metadata": {
        "id": "No2GRvTpnLBl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##12. loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "##13. optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "9-zuJLeUnQLB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##14. 출력 크기 점검\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-RxRaiHnh9U",
        "outputId": "428d7e1d-29d5-476f-d5ce-5a0c8292c5f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([188, 10, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##15. Training 시작\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    ##문제(7) : outputs, Y 형태 그대로 넣으면 안되죠. view 함수를 이용해 loss값을 계산해봅시다.\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #16. 예측결과 확인\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
        "            predict_str += ''.join([world_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += world_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxxrxCd2nwoo",
        "outputId": "2b2a03a3-6ccc-4d0d-cc7e-a7cac44a2b54"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knpb'bnkkkbnknnnkbnnnnbnnnpbnnppnbnknnnknnnpnpnknnknknnnnbnnpkbknnkbnpb'bnkkkbnknnnb'bnnnnnnknb'knpp'bnknnpnbnnnncnnnnp'bnknnpnknknknpkbknnkbnpb'bnkkkbnknnnkbnnnnknbbnnpnkpnknpnkbnkknpnnpnnnbknnnnp\n",
            "            k   k     k            k                k          k               k      k     k       '     'wk             k             k               k   k     k   k          k                   \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "      a    ae    e    a                         e   ae    ee    e   e    ae   ae                            e   a                   e    e   e    ae   ae    e                         ae       e    \n",
            "      ae   aee  aee   ae    e      e        e   ee  ae   aee   ae   e    ae   aee   eae     ee      e      aee  ae   e     e   ae   ae  ae   e    ae   aee  aee   ae  e   e      ae    ae      ee    \n",
            "      ae   aee  aae   ae  aae   ae e   aee  e   ee  ae   aee   ae   ee   ae   aee   aae    aee      e      aee  ae   ee    ee  ae   ae  aee  ee   ae   aee  aae   ae  e   ee     ae   aae     tae    \n",
            "     tae   aee  aae   ae  aae   ae e   aee  e  tee  he   ahe   ae   ee  tae   aee   hae    ahe      e   e  aee  ae   ae    ee  ae   ae  ae   ee  tae   aee  aae   he  e   ae     aee  aae     the    \n",
            "     tae   ahe  aae  tae  ahe      e   a    e  the  he   ahe  the   e   tae   ahe   hae    the      e      ahe  ae   at    ee  ae   ae  he   e   tae   ahe  aae  the      ee     ae   tae     the    \n",
            "     tae   ahe  ha   tae    e      e        e  th  the   the  the   e   tae   ahe   hth    th       e      ahe  h     t        ae   a   he   e   tae   ahe  ha   the             ae   tae     the    \n",
            "     tae   ahe  he   ta     e      h        e  th  the   the  the   e   tae   ahe    th    th          h   ah   h     te  h    ae   at the   e   tae   ahe  he   th          e   ae   taet    the    \n",
            " e   tal   ahe  he   th   h       th       a   th  the   the  the  he   tal   ahe    the   th  e       h   ah  tht    te th   tal  hat the  he   tal   ahe  he   th   e  t  te   ae   taete   the    \n",
            " h   tal   aoe th    to   h e     th       a   th  the   thet the  he   tal   aoe    the   th th   t  th   aol tht    te th t tal  hat the  he   tal   aoe th    th the  t  th   ar   taeth   thet   \n",
            " a   tol   aoe th    to   r e t   ah  he   a   th  ahet  thet thetthe   tol   aoe    the   th th   te th   aol tht  thth thet tal  hat thetthe   tol   aoe th    th thet th to  har   tolto   thet   \n",
            " a   tol   aoe th te to  arte t   ar  ho   ae  th  ahet  thet thetthe   tol   aoe    thete th th  at  th   aol tht  thth thet tal that thetthe   tol   aoe th te th thet t  to  har   toeto   thet  t\n",
            " a   tal   aoe th te to  ar e t   ar  ho   a   th  ahet  thet thetthe   tal   aoe    thet  th to  at  ta   aoe tot    th thet tal that thetthe   tal   aoe th te th toet w  to  aar   toeto   thet   \n",
            " a   tai   aae the e to     e t   ar   o   a   th  ahet  thet the the   tai   aae    thet  th to      aa   aoe to     th that tal  hat the the   tai   aae the e th toet w  to  war   ta to   thet   \n",
            " ae  tai   aae the   to     e t   ar   o   a   th  thet  thet the the   tai   aae    the   th to      aa   aae th     te that  al  hat the the   tai   aae the   th toet w  to  war   ta to   thet   \n",
            " ae  tai   aae the   to     e t   ar  ta   a   th  thel  thet the thel  tal   aae    the   th to   a  aa   aae th     te thal  al that the thel  tal   aae the   th taet a  to  war   ta to   thel   \n",
            " ae  tal   aae the   to   aae t   ar  ta   ae  th  thel  thet the thec  tal   aae    the   th to   a  aa   aae tht    te thal aal that the thec  tal   aae the   th taet a  ta  aarl  taeto   thel   \n",
            " ae  tal   aae the   to  atae t   ar  ta  tae  th  ahel  thet the thic  tal   aae    the   th to  aa  au   aae tht    te thal aal that the thic  tal   aae the   th taet a  ta  aarl  taetot  thel  t\n",
            " ae  tal   are the e to  atae t   ar  to  tae  th  ahel  thet the thic  wal   are    the   th to  aa  au   aal tht   ete thel aal thet the thic  wal   are the e th taet a  aat aarl  ta tot  thel  t\n",
            " ae  tal   are the e to  athe t   ar  to  ta   th  ahel  thet the thick wal   are    the e th to  aa  au   aal tht e ete thel aal thet the thick wal   are the e th taet u  aat aarl  ta tot  thet  t\n",
            "raek tal   are there to  athe t   ar  to  ta   th  ahet  thet the thick wal   are    the e th te   a  au   aal tht e ets thel aal thet the thick wal   are there th taet u  aat aarl  ta tot  thet   \n",
            "reek wall  are there to  athe t   ar  te  ta   th  thet  thet therthick wall  are    there th te   a  au   tal tht e ets thes aal thet therthick wall  are there th thet u  aat warl  ta tot  thet   \n",
            "reek wall  are there to  athe t   ar  te  ta   th  thet  thet therthick wall  are    there th to   a  au   tal tot e ets thes tal thet therthick wall  are there th thet u  aet warl  ta tot  thet   \n",
            "reek wall  are there tol athe t   ar  ta  ta   th  thet  whet therthick wall  are    there wh to   a  au   tal tot e ets thes tal thet therthick wall  are there th thet u  aet warl  ta tot  thet   \n",
            "reck wall  are there wol athe t   arl ta  wa   th  thet  whet therthick wall  are    there wh to   a  au   tal tot e ets thes tal thet therthick wall  are there wh thet u  aet warl  ta tol  thet   \n",
            "reck wall  are there wor atte t   arl tat wa   th  thet  whet thertaick wall  are    there whtte   a  au   tal tot e ets thes tal ther thertaick wall  are there whtthet u  aet uarl  ta tol  thet   \n",
            "reck wall  are there wor atte t   arl tat wa   th  thes  whet thertaick wall  are    there whtte   u  au   tat tothe eas thes tal ther thertaick wall  are there whtthet u  aet uarl  ta tot  thet   \n",
            "reck wall  are there wor atte t   arl tau wa   th  thet  whet therbaick wall  are    there whtte   u  au   tot tothe eas thes tal ther therbaick wall  are there whtthet u  aet uarl  ta tot  thet   \n",
            "reck wall  are there wor atae t   arl tau wus  th  thet  whet therbaick wall  are    there whtte   u  au   tot tothe eas thes tal ther therbaick wall  are there whtthet u  aet uarl  ta tot  thet   \n",
            "reck wall  are there tor atae to  arl tau wult tht thet  whet therbaick wall  are t  there thtte   u  au   tot tothe eas then tal thet therbaick wall  are there thtthet u  uet uarl  ta tols thet   \n",
            "reck wall  are there tor atae to  arl tau wult tht thet  whet therbaick wall  are st there thtte   us hu   tht tothe eas then tal thet therbaick wall  are there thtthet us uet uarl  ta tols thet   \n",
            "rick wall  are there tor atre t   arl tau wult tht thes  whet the trick wall  are st the e tht e   us hu   bht tot e eas then tal thet the trick wall  are there tht hew us hew uarls ta tols then   \n",
            "rick walls are there tor atre t   arl tau wult tht thes  whet the trick walls are st the e tht e   us hut  but tot e eas then tal thet the trick walls are there tht hew us hew uarls ta tols then   \n",
            "rick walls are there tor a to to  arl tau wust tht thes  whet the trick walls are st the e th  e   us hut  buthtot ereas then tal thet the trick walls are there tht haw us hew uarls wa tols then   \n",
            "rick walls are there tor a to to  arl tou wust tht bhes  whet the trick walls are st the e th  e   us hut  buthtot ereas thin tal thet the trick walls are there th  haw us hew uarls wa tols thenk  \n",
            "rick walls are there tor a to to  arl  ou wust tot bhes  whet the trick walls are st there th  e   us huth buthtot ereas thin tal that the trick walls are there th  haw us hew ualll wa tals thenk  \n",
            "rick walls are there tor a ro to  a l you wust tot thes  whet the trick walls are st there th  e   us huth but tot ereas thin tal that the trick walls are there th  haw us hew ualll wa tans thenk  \n",
            "rick walls are there tor a re to  a l you wust tot thes  whet the trick walls are st there th  e   us hut  aut tet ereas thin tal that the trick walls are there th  haw us hew ualll wa tans thenk  \n",
            "rick walls are there tor a re to  a l you must tot thes  whet the trick walls are kt there th  e   us huth aut tet ereas thin tal that the trick walls are there th  haw us hew ualll wa tans thenk  \n",
            "rick walls are there tor a reeto  a l you must tot then  whet the trick walls are kt there th  e   us huth but tet ereas thin tal that the trick walls are there th  haw us hew ualll wa tans thenk  \n",
            "rick walls are there tor a reeto  a l you must tot thenk whet the trick walls are kt there th  e   us huth but rethereas thin tal that the trick walls are there th  haw us hew ualll we tant thenk  \n",
            "rick walls are there tor a reato  a l you must tot thenk whet the trick walls are kt there th te p us huth but rethereas thin tal that the trick walls are there th thaw us hew ualll we tant thenk  \n",
            "rick walls are there tor a reato  a l you must tot thenk what the brick walls are kt there th te p us huth but rethereas thin tal that the brick walls are there th thaw us how ualll we tant thenks \n",
            "rick walls are there tor a reaton and you must tot thenk what the brick walls are kt there th te p us huth buthrethereas thin tal that therbrick walls are there th thaw us how ualll we tant thenks \n",
            "rick walls are there tor a reason and you must tot thenk what the brick walls are 't there th te p us huth buthrethereas thin tal that therbrick walls are there th toaw us how ualll we want thenks \n",
            "rick walls are there tor a reason and you must tot thenk what the brick walls are 't there th te p us huth buthrethereas thin tal that the brick walls are there th toow us how uadly we want thenks \n",
            "rick walls are there tor a reason and you must tot thenk what the brick walls are 't there th te p us huth but retherean thin tal that the brick walls are there th toow us how uadly we want thenks \n",
            "rick walls are there tor a reason and you must tot thenk what the brick walls are 't there th teap us huth but retherean thin wal that therbrick walls are there th toow us how uadly we want thenks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there th teap us outh but retherean thin wal that therbrick walls are there th toow us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there th seap us outh but rether an thin way that therbrick walls are there th show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there th seap us outh but rether an thin way that the brick walls are there to show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there th seap us outh but rether an thin way that the brick walls are there to show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seap us outh but rether an thin way that the brick walls are there to show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us outh but rether an thin way that the brick walls are there to show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us outh but rether an thin way that the brick walls are there to show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us outh but rether an thin way that the brick walls are there to show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us outh but rether an thin way that the brick walls are there to show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rether an thin way that the brick walls are there to show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rether an thin way that the brick walls are there to show us how uadly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rether an thin way that the brick walls are there to show us how badly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rether an thin way that the brick walls are there to show us how badly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rether an thin way that the brick walls are there to show us how badly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rether an thin way that the brick walls are there to show us how badly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rather an thin way that the brick walls are there to show us how badly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rather in thin way that the brick walls are there to show us how badly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rather in thin way that the brick walls are there to show us how badly we want thinks \n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rather in thin way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rather in thin way that the brick walls are there to show us how badly we want thinktw\n",
            "rick walls are there tor a reason and you must tot think that the brick walls are 't there to seep us out, but rather in thin way that the brick walls are there to show us how badly we want thinktw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in thin way that the brick walls are there to show us how badly we want thinktw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in thin way that the brick walls are there to show us how badly we want thinktw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "8qUkbiw2t0Il",
        "outputId": "7e118718-4948-44df-ff73-34b4b19dd581"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksw\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과가 어떤가요?? 마지막 에폭의 문장이 그럴싸한가요?"
      ],
      "metadata": {
        "id": "PkIzDTdyvTHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task2\n",
        "\n",
        "위 sentence는 제가 임의로 생성한 문장들입니다.\n",
        "\n",
        "마음에 드시는 문구 가져오셔서 문장이 어떻게 생성되는지 확인해보세요! \n",
        "\n",
        "영어가 아닌 한국어로 시도해보는 것도 좋겠죠? \n",
        "\n",
        "수정이 많이 필요(토큰화 등) 할 수 있으나 한번 시도해보시는 것 권장드립니다 :)\n",
        "\n",
        "위 베이스라인은 어디든 수정하셔도 좋고 조금 더 자연스러운 문장이 나올 수 있게 다양한 시도를 해보세요!\n",
        "\n",
        "조건 : 문장 3개 이상, 연결성이 있는 문장을 \" \" 으로 구분하여 ( )에 넣기"
      ],
      "metadata": {
        "id": "kN1zL8Dpvane"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 생성할 문장 데이터\n",
        "\n",
        "sentence = (\"위에 써 있는 문장들은 제가 임의로 생성한 문장들입니다 \"\n",
        "            \"마음에 드는 문구를 가져와서 문장이 어떻게 생성 되는지 확인해 보세요 \"\n",
        "            \"수정이 많이 필요할 수 있으나 여러 번 시도해 보시는 것을 권장드립니다. \")"
      ],
      "metadata": {
        "id": "eINzToLonkd-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  \n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8QDIKqvuyk8",
        "outputId": "25bf1a64-14d2-479a-8cca-9ee92d2e0956"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['위에', '써', '있는', '문장들은', '제가', '임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이', '어떻게', '생성', '되는지', '확인해', '보세요', '수정이', '많이', '필요할', '수', '있으나', '여러', '번', '시도해', '보시는', '것을', '권장드립니다', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 문제(1): 각 문자에 정수 인코딩 (공백도 하나의 원소로 포함)\n",
        "vocab = {char: idx for idx, char in enumerate(tokens)}"
      ],
      "metadata": {
        "id": "199u0qPRnkd_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36bb0799-d8f3-459a-ff9c-3bba59c8df51",
        "id": "Bvb5Auhinkd_"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'위에': 0, '써': 1, '있는': 2, '문장들은': 3, '제가': 4, '임의로': 5, '생성한': 6, '문장들입니다': 7, '마음에': 8, '드는': 9, '문구를': 10, '가져와서': 11, '문장이': 12, '어떻게': 13, '생성': 14, '되는지': 15, '확인해': 16, '보세요': 17, '수정이': 18, '많이': 19, '필요할': 20, '수': 21, '있으나': 22, '여러': 23, '번': 24, '시도해': 25, '보시는': 26, '것을': 27, '권장드립니다': 28, '.': 29}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. 문자 집합 크기 확인\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('문자 집합 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058e7c58-9147-4673-afec-366876c2f0f9",
        "id": "-Ja53UQ5nkd_"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 크기 : 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
        "\n",
        "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
        "sequence_length = 10  # 너무 길거나 너무 짧게 잡으면 안됩니다!\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "l3G6xfZnnkd_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. seqence 길이 단위 자르기\n",
        "\n",
        "# 데이터 구성을 위한 리스트\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "## 문제(2): 반복문 내에서의 인덱싱을 사용하여 sequence_length 값 단위로 샘플을 잘라 데이터 만들기, y_str은 x_str은 한 칸씩 쉬프트된 sequnce\n",
        "\n",
        "for i in range(0, len(tokens) - sequence_length):\n",
        "  x_str = tokens[i:i+sequence_length]\n",
        "  y_str = tokens[i+1:i+sequence_length+1]\n",
        "  print(i, x_str, \"->\", y_str)\n",
        "\n",
        "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
        "  x_data.append([vocab[c] for c in x_str])\n",
        "  y_data.append([vocab[d] for d in y_str])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f1173f-db35-4fd5-d2ff-4622a9cb54a6",
        "id": "8_0qDGoRnkeA"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ['위에', '써', '있는', '문장들은', '제가', '임의로', '생성한', '문장들입니다', '마음에', '드는'] -> ['써', '있는', '문장들은', '제가', '임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를']\n",
            "1 ['써', '있는', '문장들은', '제가', '임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를'] -> ['있는', '문장들은', '제가', '임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서']\n",
            "2 ['있는', '문장들은', '제가', '임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서'] -> ['문장들은', '제가', '임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이']\n",
            "3 ['문장들은', '제가', '임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이'] -> ['제가', '임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이', '어떻게']\n",
            "4 ['제가', '임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이', '어떻게'] -> ['임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이', '어떻게', '생성']\n",
            "5 ['임의로', '생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이', '어떻게', '생성'] -> ['생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이', '어떻게', '생성', '되는지']\n",
            "6 ['생성한', '문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이', '어떻게', '생성', '되는지'] -> ['문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이', '어떻게', '생성', '되는지', '확인해']\n",
            "7 ['문장들입니다', '마음에', '드는', '문구를', '가져와서', '문장이', '어떻게', '생성', '되는지', '확인해'] -> ['마음에', '드는', '문구를', '가져와서', '문장이', '어떻게', '생성', '되는지', '확인해', '보세요']\n",
            "8 ['마음에', '드는', '문구를', '가져와서', '문장이', '어떻게', '생성', '되는지', '확인해', '보세요'] -> ['드는', '문구를', '가져와서', '문장이', '어떻게', '생성', '되는지', '확인해', '보세요', '수정이']\n",
            "9 ['드는', '문구를', '가져와서', '문장이', '어떻게', '생성', '되는지', '확인해', '보세요', '수정이'] -> ['문구를', '가져와서', '문장이', '어떻게', '생성', '되는지', '확인해', '보세요', '수정이', '많이']\n",
            "10 ['문구를', '가져와서', '문장이', '어떻게', '생성', '되는지', '확인해', '보세요', '수정이', '많이'] -> ['가져와서', '문장이', '어떻게', '생성', '되는지', '확인해', '보세요', '수정이', '많이', '필요할']\n",
            "11 ['가져와서', '문장이', '어떻게', '생성', '되는지', '확인해', '보세요', '수정이', '많이', '필요할'] -> ['문장이', '어떻게', '생성', '되는지', '확인해', '보세요', '수정이', '많이', '필요할', '수']\n",
            "12 ['문장이', '어떻게', '생성', '되는지', '확인해', '보세요', '수정이', '많이', '필요할', '수'] -> ['어떻게', '생성', '되는지', '확인해', '보세요', '수정이', '많이', '필요할', '수', '있으나']\n",
            "13 ['어떻게', '생성', '되는지', '확인해', '보세요', '수정이', '많이', '필요할', '수', '있으나'] -> ['생성', '되는지', '확인해', '보세요', '수정이', '많이', '필요할', '수', '있으나', '여러']\n",
            "14 ['생성', '되는지', '확인해', '보세요', '수정이', '많이', '필요할', '수', '있으나', '여러'] -> ['되는지', '확인해', '보세요', '수정이', '많이', '필요할', '수', '있으나', '여러', '번']\n",
            "15 ['되는지', '확인해', '보세요', '수정이', '많이', '필요할', '수', '있으나', '여러', '번'] -> ['확인해', '보세요', '수정이', '많이', '필요할', '수', '있으나', '여러', '번', '시도해']\n",
            "16 ['확인해', '보세요', '수정이', '많이', '필요할', '수', '있으나', '여러', '번', '시도해'] -> ['보세요', '수정이', '많이', '필요할', '수', '있으나', '여러', '번', '시도해', '보시는']\n",
            "17 ['보세요', '수정이', '많이', '필요할', '수', '있으나', '여러', '번', '시도해', '보시는'] -> ['수정이', '많이', '필요할', '수', '있으나', '여러', '번', '시도해', '보시는', '것을']\n",
            "18 ['수정이', '많이', '필요할', '수', '있으나', '여러', '번', '시도해', '보시는', '것을'] -> ['많이', '필요할', '수', '있으나', '여러', '번', '시도해', '보시는', '것을', '권장드립니다']\n",
            "19 ['많이', '필요할', '수', '있으나', '여러', '번', '시도해', '보시는', '것을', '권장드립니다'] -> ['필요할', '수', '있으나', '여러', '번', '시도해', '보시는', '것을', '권장드립니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb132cd-1f16-479c-b2f4-5196e4b54b57",
        "id": "tncgdHQhnkeA"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
        "\n",
        "## 문제(4) : x_data를 원핫인코딩 > numpy의 eye를 쓸 수 있지 않을까?\n",
        "x_one_hot = [np.eye(len(vocab))[x] for x in x_data]\n",
        "\n",
        "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
        "\n",
        "## 문제(5) : x_one_hot과 y_data 텐서로 변환 : 둘 다 같은 형식의 텐서로 변환하면 될까?? (FloatTensor, LongTesor 중 맞는 것은?)\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "xJeGRwchnkeA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##8. 크기 확인\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a252f33-02a9-4023-87c4-9ac3235ecdd0",
        "id": "HOu21K17nkeA"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([20, 10, 30])\n",
            "레이블의 크기 : torch.Size([20, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##9.원핫인코딩 결과 샘플 확인하기\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f8339e-0e30-4122-950f-d8f6986c84a4",
        "id": "NERnLZUWnkeA"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##10. 레이블 데이터 샘플 확인하기\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89ca935-8e4f-4e01-a8b2-10b6f0e3e80a",
        "id": "W-YT_ni9nkeA"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##11. RNN 모델 구현\n",
        "\n",
        "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
        "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
        "### batch_fisrt 설정 필요할까? (유튜브 강의 참고)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, input_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    rnn_out, hidden = self.rnn(x)\n",
        "    x= self.fc(rnn_out)\n",
        "    return x"
      ],
      "metadata": {
        "id": "HNFIn5FFnkeA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 2)"
      ],
      "metadata": {
        "id": "mWOq_qNRnkeB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##12. loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "##13. optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "HaB7N7qOnkeB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##14. 출력 크기 점검\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1e83e1-d42b-492a-b442-36c8add23b2a",
        "id": "J5bCEl0snkeB"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 10, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##15. Training 시작\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    ##문제(7) : outputs, Y 형태 그대로 넣으면 안되죠. view 함수를 이용해 loss값을 계산해봅시다.\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #16. 예측결과 확인\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
        "            predict_str += ''.join([world_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += world_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "84e6221c-c912-4589-835e-e51eed93f3c6",
        "id": "eHa3FXZ_nkeB"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hhBhthhhhhhhhBhhtthhhhhhhthhh\n",
            "hhhhrhhohhohrrhhhhrhhhhhhhhhh\n",
            "hhrhrhhokhohrkhrrkrhrhhhhhhrh\n",
            "hrrhrkhokhotuchlrkrhrhhkrkhrk\n",
            "grrhrkhokhotucglrkrhrhhkkkkrk\n",
            "grrkrkhykhotucglriotrkgekkguc\n",
            "glBkrkhykhotucglciotrcgekcguc\n",
            "glgiuk,ykhotucglciotucgekcguc\n",
            "glyiuk,ykhotucgl'ioBucgeucguc\n",
            "glyiuk,ykhotucgl'ioBucgeucgec\n",
            "gcyiuk,ykhotucgl'ibBrapeucgec\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-bd60ed0536fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mpredict_str\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworld_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 그 다음에는 마지막 글자만 반복 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mpredict_str\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mworld_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "id": "X9Vz6IgbnkeB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}